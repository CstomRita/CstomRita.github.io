@autoHeader: 2.1.1.1.1.1

<p align="right">update time : {docsify-updated}</p>

## 分布式系统基本概念

### 分布式系统的主要特征

- **分布性**

分布式系统中的多台计算机之间在空间位置上可以随意分布，同时，机器的分布情况也会随时变动。

- **对等性**

分布式系统中的计算机没有主／从之分，即没有控制整个系统的主机，也没有被控制的从机，组成分布式系统的所有计算机节点都是对等的。副本（Replica）是分布式系统最常见的概念之一，指的是分布式系统对数据和服务提供的一种冗余方式。在常见的分布式系统中，为了对外提供高可用的服务，我们往往会对数据和服务进行副本处理。数据副本是指在不同节点上持久化同一份数据，当某一个节点上存储的数据丢失时，可以从副本上读取该数据，这是解决分布式系统数据丢失问题最为有效的手段。另一类副本是服务副本，指多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行相应的处理。

- **自治性**

分布式系统中的各个节点都包含自己的处理机和内存，各自具有独立的处理数据的功能。通常，彼此在地位上是平等的，无主次之分，既能自治地进行工作，又能利用共享的通信线路来传送信息，协调任务处理。

- **并发性**

在一个计算机网络中，程序运行过程的并发性操作是非常常见的行为。例如同一个分布式系统中的多个节点，可能会并发地操作一些共享的资源，如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最大的挑战之一。

### 分布式系统面临的问题

- **缺乏全局时钟**

在分布式系统中，很难定义两个事件究竟谁先谁后，原因就是因为分布式系统缺乏一个全局的时钟序列控制。

- **机器宕机**

机器宕机是最常见的异常之一。在大型集群中每日宕机发生的概率为千分之一左右，在实践中，一台宕机的机器恢复的时间通常认为是24 小时，一般需要人工介入重启机器。

- **网络异常**

消息丢失，两片节点之间彼此完全无法通信，即出现了“网络分化”；消息乱序，有一定的概率不是按照发送时的顺序依次到达目的节点，考虑使用序列号等机制处理网络消息的乱序问题，使得无效的、过期的网络消息不影响系统的正确性；数据错误；不可靠的TCP，TCP 协议为应用层提供了可靠的、面向连接的传输服务，但在分布式系统的协议设计中不能认为所有网络通信都基于TCP 协议则通信就是可靠的。TCP协议只能保证同一个TCP 链接内的网络消息不乱序，TCP 链接之间的网络消息顺序则无法保证。

- **分布式三态**

如果某个节点向另一个节点发起RPC(Remote procedure call)调用，即某个节点A 向另一个节点B 发送一个消息，节点B 根据收到的消息内容完成某些操作，并将操作的结果通过另一个消息返回给节点A，那么这个RPC 执行的结果有三种状态：“成功”、“失败”、“超时（未知）”，称之为分布式系统的三态。

- **存储数据丢失**

对于有状态节点来说，数据丢失意味着状态丢失，通常只能从其他节点读取、恢复存储的状态。 *异常处理原则*：被大量工程实践所检验过的异常处理黄金原则是：任何在设计阶段考虑到的异常情况一定会在系统实际运行中发生，但在系统实际运行遇到的异常却很有可能在设计时未能考虑，所以，除非需求指标允许，在系统设计时不能放过任何异常情况。

### 衡量指标

- **性能**

系统的吞吐能力，指系统在某一时间可以处理的数据总量，通常可以用系统每秒处理的总的数据量来衡量；系统的响应延迟，指系统完成某一功能需要使用的时间；系统的并发能力，指系统可以同时完成某一功能的能力，通常也用QPS(query per second)来衡量。上述三个性能指标往往会相互制约，追求高吞吐的系统，往往很难做到低延迟；系统平均响应时间较长时，也很难提高QPS。

- **可用性**

系统的可用性(availability)指系统在面对各种异常时可以正确提供服务的能力。系统的可用性可以用系统停服务的时间与正常服务的时间的比例来衡量，也可以用某功能的失败次数与成功次数的比例来衡量。可用性是分布式的重要指标，衡量了系统的鲁棒性，是系统容错能力的体现。

- **可扩展性**

系统的可扩展性(scalability)指分布式系统通过扩展集群机器规模提高系统性能（吞吐、延迟、并发）、存储容量、计算能力的特性。好的分布式系统总在追求“线性扩展性”，也就是使得系统的某一指标可以随着集群中的机器数量线性增长。

- **一致性**

分布式系统为了提高可用性，总是不可避免的使用副本的机制，从而引发副本一致性的问题。越是强的一致的性模型，对于用户使用来说使用起来越简单。

## CAP理论

### CAP简介

**一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项**。

| 选项                              | 描述                                                         |
| --------------------------------- | ------------------------------------------------------------ |
| Consistency（一致性）             | 指数据在多个副本之间能够保持一致的特性（严格的一致性）<br/>一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据。所有节点访问同一份最新的数据。 |
| Availability（可用性）            | 指系统提供的服务必须一直处于可用的状态，每次请求都能获取到非错的响应（不保证获取的数据为最新数据） |
| Partition tolerance（分区容错性） | 能容忍网络分区，在网络断开的情况下，被分隔的节点仍能正常对外提供服务。 |

### 理解CAP

理解CAP理论最简单的方式是想象两个副本处于分区两侧，即两个副本之间的网络断开，不能通信。

- 如果允许其中一个副本更新，则会导致数据不一致，即丧失了C性质。
- 如果为了保证一致性，将分区某一侧的副本设置为不可用，那么又丧失了A性质。
- 除非两个副本可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。

首先对于分布式系统，分区是必然存在的，所谓分区指的是分布式系统可能出现的字区域网络不通，成为孤立区域的的情况。

![img](%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA.assets/fenbushi-49bf971a-63ae-4b45-bb9e-8af84dff7219.jpg)

那么分区容错性（**P**）就必须要满足，因为如果要牺牲分区容错性，就得把服务和资源放到一个机器，或者一个“同生共死”的集群，那就违背了分布式的初衷

一般来说使用网络通信的分布式系统，无法舍弃P性质，那么就只能在一致性和可用性上做一个艰难的选择。CP系统保障一致性，AP系统保障高可用。

### CAP对应的模型和应用？

#### CA without P

理论上放弃P（分区容错性），则C（强一致性）和A（可用性）是可以保证的。实际上分区是不可避免的，严格上CA指的是允许分区后各子系统依然保持CA。

CA模型的常见应用：

- 集群数据库
- xFS文件系统

#### CP without A

放弃A（可用），相当于每个请求都需要在Server之间强一致，而P（分区）会导致同步时间无限延长，如此CP也是可以保证的。很多传统的数据库分布式事务都属于这种模式。

CP模型的常见应用：

- 分布式数据库
- 分布式锁

#### AP wihtout C

要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的NoSQL都属于此类。

AP模型常见应用：

- Web缓存
- DNS

在熟悉的注册中心`ZooKeeper`、`Eureka`、`Nacos`中：

- ZooKeeper 保证的是 CP
- Eureka 保证的则是 AP
- Nacos 不仅支持 CP 也支持 AP

### CAP理论后续

在CAP理论提出十二年之后，其作者又出来辟谣。“三选二”的公式一直存在着误导性，它会过分简单化各性质之间的相互关系:

- 首先，由于分区很少发生，那么在系统不存在分区的情况下没什么理由牺牲C或A。
- 其次，C与A之间的取舍可以在同一系统内以非常细小的粒度反复发生，而每一次的决策可能因为具体的操作，乃至因为牵涉到特定的数据或用户而有所不同。
- 最后，这三种性质都可以在程度上衡量，并不是非黑即白的有或无。可用性显然是在0%到100%之间连续变化的，一致性分很多级别，连分区也可以细分为不同含义，如系统内的不同部分对于是否存在分区可以有不一样的认知。

所以一致性和可用性并不是水火不容，非此即彼的，可以在一致性和可用性之间做到了很好的平衡。

##  BASE 理论

### BASE理论是什么

**BASE** 是 **Basically Available（基本可用）** 、**Soft-state（软状态）** 和 **Eventually Consistent（最终一致性）** 三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，它大大降低了我们对系统的要求。

#### 基本可用

基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。

比如：

- **响应时间上的损失**：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒
- **系统功能上的损失**：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面

####  软状态

软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。

#### 最终一致性

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

### BASE和CAP的关系

BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的结论，是基于CAP定理逐步演化而来的.

其基本思路就是：<font color=red>**通过业务，牺牲强一致性而获得可用性，并允许数据在一段时间内是不一致的，但是最终达到一致性状态。**</font>

### BASE和ACID的关系

ACID 是传统数据库常用的设计理念，追求强一致性模型。

BASE 支持的是大型分布式系统，提出通过牺牲强一致性获得高可用性。

ACID 和 BASE 代表了两种截然相反的设计哲学，在分布式系统设计的场景中，系统组件对一致性要求是不同的，因此 ACID 和 BASE 又会结合使用。

## 分布式一致性算法 //todo

数据不能存在单个主机节点上，否则会出现单点故障。一致性算法的目的是保证在分布式系统中，多数据副本节点数据一致性。

一致性分为强一致性、弱一致性：

强一致性：保证系统改变提交以后立即改变集群的状态，包括Paxos、Raft（muti-paxos）、ZAB（muti-paxos）等。

弱一致性，也叫最终一致性，系统不保证改变提交以后立即改变集群的状态，但是随着时间的推移最终状态是一致的，包括DNS系统、Gossip协议等。

- - 

### 一致性Hash算法

#### 前言

设想这样的场景：

> 单个节点的缓存容量达到上限，无法继续单点增加内存，如何解决？
>
> 初步方案
>
> 增加N个缓存节点，为了保证缓存数据的均匀，一般情况会采用对key值hash，然后取模的方式，然后根据结果，确认数据落到哪台节点上：如`hash(key)%N `
>
> 很好，这个的确解决了上面的问题，实现了初步的分布式缓存，数据均匀分散到了各个节点上，流量请求也均匀的分散到了各个节点。
>
> 但是如果出现以下情况，会带来什么问题？
>
> 1、某台服务器突然宕机。缓存服务器从N变为N-1台。
>
> 2、缓存容量达到上限或者请求处理达到上限，需要增加缓存服务器，假定增加1台，则缓存服务器从N变为N+1
>
> 上面的情况带来的问题：
>
> 增加或者删除缓存服务器的时候，意味着大部分的缓存都会失效。这个是比较致命的一点，缓存失效，如果业务为缓存不命中，查询DB的话，会导致一瞬间DB的压力陡增。可能会导致整个服务不可用。

我们希望的情况是：增删机器时，希望大部分key依旧在原有的缓存服务器上保持不变。举例来说：key1,key2,key3原先在Cache1机器上，现在增加一台缓存服务器，希望key1,key2,key3依旧在Cache1机器上，而不是在Cache2机器上。 

一致性哈希算法即为改进方案，主要使用在分布式数据存储系统中，按照一定的策略将数据尽可能均匀分布到所有的存储节点上去，使得系统具有良好的负载均衡性能和扩展性。

#### 算法简介

一致性哈希算法在1997年由麻省理工学院提出的一种分布式哈希(DHT)实现算法，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。

一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得分布式哈希(DHT)可以在P2P环境中真正得到应用。

一致性hash算法提出了在动态变化的Cache环境中，判定哈希算法好坏的四个定义:

- `平衡性(Balance)`: 平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。
- `单调性(Monotonicity)`: 单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到原有的或者新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。
- `分散性(Spread)`: 在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。
- `负载(Load)`: 负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同 的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。

#### 简单一致性哈希

##### 哈希环

常见的hash算法可以把一个key值哈希到一个具有2^32个桶的空间中。也可以理解成，将key值哈希到 [0, 2^32) 的一个数字空间中。 我们假设这个是个首尾连接的环形空间。

![img](%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA.assets/alg-dist-hash-1.jpg)

现在有key1,key2,key3,key4 4个key值，我们通过一定的hash算法，将其对应到上面的环形hash空间中。

```bash
k1=hash(key1);
k2=hash(key2);
k3=hash(key3);
k4=hash(key4);
```

![img](%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA.assets/alg-dist-hash-2.jpg)

同样的，假设我们有3台cache服务器，把缓存服务器通过hash算法，加入到上述的环中。一般情况下是根据机器的IP地址或者唯一的计算机别名进行哈希。

```bash
c1=hash(cache1);
c2=hash(cache2);
c3=hash(cache3);
```

![img](%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA.assets/alg-dist-hash-3.jpg)

接下来就是数据如何存储到cache服务器上了，key值哈希之后的结果顺时针找上述环形hash空间中，距离自己最近的机器节点，然后将数据存储到上面，k1 存储到 c3 服务器上， k4,k3存储到c1服务器上， k2存储在c2服务器上。

![img](%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA.assets/alg-dist-hash-4.jpg)

##### 删除节点

假设cache3服务器宕机，这时候需要从集群中将其摘除。那么，之前存储再c3上的k1，将会顺时针寻找距离它最近的一个节点，也就是c1节点，这样，k1就会存储到c1上了。

![img](%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA.assets/alg-dist-hash-5.jpg)

摘除c3节点之后，只影响到了原先存储再c3上的k1，而k3、k4、k2都没有受到影响，也就意味着解决了最开始的解决方案(hash(key)%N)中可能带来的雪崩问题。

##### 增加节点

新增C4节点之后，原先存储到C1的k4，迁移到了C4，分担了C1上的存储压力和流量压力。

![img](%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA.assets/alg-dist-hash-6.jpg)

#### 虚拟节点优化

##### 简单一次性哈希的弊端

上面的简单的一致性hash的方案在某些情况下但依旧存在问题: 一个节点宕机之后，数据需要落到距离他最近的节点上，会导致下个节点的压力突然增大，可能导致雪崩，整个服务挂掉。

![img](%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA.assets/alg-dist-hash-7.jpg)

当节点C3摘除之后，之前再C3上的k1就要迁移到C1上，这时候带来了两部分的压力:

- 之前请求到C3上的流量转嫁到了C1上,会导致C1的流量增加，如果之前C3上存在热点数据，则可能导致C1扛不住压力挂掉。
- 之前存储到C3上的key值转义到了C1，会导致C1的内容占用量增加，可能存在瓶颈。

当上面两个压力发生的时候，可能导致C1节点也宕机了。那么压力便会传递到C2上，又出现了类似滚雪球的情况，服务压力出现了雪崩，导致整个服务不可用。这一点违背了最开始提到的四个原则中的 `平衡性`， 节点宕机之后，流量及内存的分配方式打破了原有的平衡。

##### 虚拟节点

“虚拟节点”( virtual node )是实际节点(机器)在 hash 空间的复制品( replica )，一个实际个节点对应了若干个虚拟节点，这个对应个数也成为复制个数，虚拟节点在 hash 空间中以hash值排列。

假设存在以下的真实节点和虚拟节点的对应关系。

```bash
Visual100—> Real1
Visual101—> Real1
Visual200—> Real2
Visual201—> Real2
Visual300—> Real3
Visual301—> Real3
```

同样的，hash之后的结果如下:

```bash
hash(Visual100)—> V100  —> Real1
hash(Visual101)—> V101  —> Real1
hash(Visual200)—> V200  —> Real2
hash(Visual201)—> V201  —> Real2
hash(Visual300)—> V300  —> Real3
hash(Visual301)—> V301  —> Real3
```

![img](%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA.assets/alg-dist-hash-8.jpg)

假设Real1机器宕机，则会发生一下情况。

- 原先存储在虚拟节点V100上的k1数据将迁移到V301上，也就意味着迁移到了Real3机器上。
- 原先存储再虚拟节点V101上的k4数据将迁移到V200上，也就意味着迁移到了Real2机器上。

由此，某个节点宕机之后，存储及流量压力并没有全部转移到某台机器上，而是分散到了多台节点上。解决了节点宕机可能存在的雪崩问题。当物理节点多的时候，虚拟节点多，这个的雪崩可能就越小。

#### 应用场景

##### 缓存集群，如memcache

memcached在实现分布式群集部署时，memcached服务之间是不能进行通讯的，分布式也是通过客户端的算法吧数据保存在不同的memcached中，所以当我们做完群集客户端往里面写入数据时，会出现下面的情况。客户端往一个memcached节点写入数据后，另外两个节点是查询不到的，此时就需要用一致性hash算法。

首先求出memcached 服务器（节点）的哈希值，并将其配置到0～232的圆（continuum）上。然后用同样的方法求出存储数据的键的哈希值，并映射到圆上。然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过232 仍然找不到服务器，就会保存到第一台memcached 。

> 但是memcache还是有问题的：
>
> memcached对容错不做处理的，也就是说当Memcached 服务端挂掉或者 Memcached 重启复活，由于Memcached重启后无数据，对高并发的大型系统中，是没办法接受的，我们在集群的情况下，要做到负载均衡和容错 ，这种情况下， 服务器端挂掉，负载就不那么均衡，缓存数据也会丢失 ，重新对数据库进行操作，加大的系统的性能， 数据丢失容错也没做得那么好。
>
> Memcache自身并没有实现集群功能，如果想用Memcahce实现集群需要借助第三方软件或者自己设计编程实现，如repcached、memagent、 memcached-ha等。

##### Redis集群模式

Redis集群模式使用到了哈希一致性算法。

(1)所有的redis节点彼此互联(PING-PONG机制)，内部使用二进制协议优化传输速度和带宽.

(2)节点的fail是通过集群中超过半数的节点检测失效时才生效。

(3)客户端与redis节点直连，不需要中间proxy层，客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可。

(4)redis-cluster把所有的物理节点映射到[0-16383]slot（插槽）上，cluster 负责维护node<->slot<->value。

具体可见redis集群模式。

#### 思考问题

##### 为什么需要哈希环？

 为了保证节点宕机摘除之后，原先存储在当前节点的key能找到可存储的位置。

举个极端的例子，在不是环状hash空间下，刚好缓存的服务器处于0这个位置，那么0之后是没有任何节点信息的，那么当缓存服务器摘除的时候，以前存储在这台机器上的key便找不到顺时针距离它最近的一个节点了。

但如果是环形空间，0之后的最近的一个节点信息有可能是2^32-1这个位置，他可以找到0之后的节点。

### Paxos算法 //todo

https://pdai.tech/md/arch/arch-z-theory.html#分布式一致性算法)



### Raft算法 //todo



### ZAB算法 //todo





