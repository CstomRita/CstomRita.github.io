@autoHeader: 2.1.1.1.1.1

<p align="right">update time : {docsify-updated}</p>

> [!Note] 
>
> - 大文件，无法一次读取问题【分治思想】
>
> 1. 分而治之，进行哈希取余或者其他筛选条件，大文件分散成多个小文件
>
> 2. 分析小文件，确定目标在哪些文件中，单独对小文件处理
>
> - 大文件topK问题【分治思想+大小顶堆】
>
> 1. 分而治之，进行哈希取余；
> 2. 使用 HashMap 统计频数；
> 3. 求解**最大**的 TopN 个，用**小顶堆**；求解**最小**的 TopN 个，用**大顶堆**
>
> - 存在/不存在/不重复元素【位图法】
>
> 1. 用bit表示状态，例如1位bit用0和1表示；2位bit可以用00 表示这个没出现过；01 表示这个出现过一次；10 表示这个出现了多次
> 2. 在遍历过程中修改状态位。
> 3. 位图法的内存占用为（N*状态位数）
>
> - 字符串类问题【可考虑前缀树】
>
> 1. 前缀树经常被用来统计字符串的出现次数。树的结点保存字符串出现次数，0 表示没有出现。
> 2. 字符串查找，判断是否有重复的字符串等。

## top K 问题

### 数组top K最大/小元素

> 在未排序的数组中找到第 **k** 个最大的元素

#### 【解法一】全局排序，取K个数

我们能想到的最简单的就是将数组进行排序（可以是最简单的快排），取前 K 个数就可以了

**复杂度分析：**

- 时间复杂度：O(nlogn)
- 空间复杂度：O(logn)

#### 【二】局部冒泡排序

可以使用冒泡排序，每次将最大的数在最右边冒泡出来，只冒泡 k次即可。

![img](%E5%9C%BA%E6%99%AF%E7%AE%97%E6%B3%95%E9%A2%98.assets/687474703a2f2f7265736f757263652e6d757969792e636e2f696d6167652f32303230303632383233353533302e676966.gif)

**复杂度分析：**

- 时间复杂度：最好时间复杂度 O(n)，平均时间复杂度 O(n*k)
- 空间复杂度：O(1)

#### 【解法三】构造前 `k` 个最大元素小顶堆，取堆顶

我们也可以通过构造一个前 `k` 个最大元素小顶堆来解决，小顶堆上的任意节点值都必须小于等于其左右子节点值，即堆顶是最小值。

所以我们可以从数组中取出 `k` 个元素构造一个小顶堆，然后将其余元素与小顶堆对比，如果大于堆顶则替换堆顶，然后堆化，所有元素遍历完成后，堆中的堆顶即为第 `k` 个最大值

具体步骤如下：

1. 从数组中取前 `k` 个数（ `0` 到 `k-1` 位），构造一个小顶堆
2. 从 `k` 位开始遍历数组，每一个数据都和小顶堆的堆顶元素进行比较，如果小于堆顶元素，则不做任何处理，继续遍历下一元素；如果大于堆顶元素，则将这个元素替换掉堆顶元素，然后再堆化成一个小顶堆。
3. 遍历完成后，堆顶的数据就是第 K 大的数据

![image-20231113193526853](%E5%9C%BA%E6%99%AF%E7%AE%97%E6%B3%95%E9%A2%98.assets/image-20231113193526853.png)

**复杂度分析：**

- 时间复杂度：遍历数组需要 O(n) 的时间复杂度，一次堆化需要 O(logk) 时间复杂度，所以利用堆求 Top k 问题的时间复杂度为 O(nlogk)
- 空间复杂度：O(k)

> 使用堆的一个优势，可以对动态数组进行top K排序：
>
> 维护一个 K 大小的小顶堆，当有数据被添加到数组中时，就将它与堆顶元素比较，如果比堆顶元素大，则将这个元素替换掉堆顶元素，然后再堆化成一个小顶堆；如果比堆顶元素小，则不做处理。这样，每次求 Top k 问题的时间复杂度仅为 O(logk)。

#### 【解法四】快速选择（只要第K个数）

无论是排序算法还是构造堆求解 Top k问题，都经过的一定量的不必要操作：

- 如果使用排序算法，我们仅仅想要的是第 k 个最大值，但是我们却对数组进行了整体或局部的排序
- 如果使用堆排序，需要维护一个大小为 `k` 的堆(大顶堆，小顶堆)，同时花费时间也很昂贵，时间复杂度为 `O(nlogk)`

快速选择算法，类似于快排思想，在每执行一次快排的时候，比较基准值位置是否在 `n-k` 位置上，

1. 如果小于 `n-k` ，则第 k 个最大值在基准值的右边，我们只需递归快排基准值右边的子序列即可；

2. 如果大于 `n-k` ，则第 k 个最大值在基准值的做边，我们只需递归快排基准值左边的子序列即可；
3. 如果等于  `n-k` ，则第 k 个最大值就是基准值

**复杂度分析：**

- 时间复杂度：平均时间复杂度O(n)，最坏情况时间复杂度为O(n<sup>2</sup>)
- 空间复杂度：O(1)

#### 【解法五】中位数BFPRT

在BFPTR算法中，仅仅是改变了快速选择（quickselect）算法中 `Partion` 中的基准值的选取，在快速选择（quickselect）算法中，我们可以选择第一个元素或者最后一个元素作为基准元，优化的可以选择随机一个元素作为基准元，而在 BFPTR 算法中，每次选择五分中位数的中位数作为基准元（也称为主元**pivot**），这样做的目的就是使得划分比较合理，从而避免了最坏情况的发生。

BFPRT 算法步骤如下：

- 选取主元
  - 将 n 个元素按顺序分为 `n/5` 个组，每组 5 个元素，若有剩余，舍去
  - 对于这 `n/5` 个组中的每一组使用插入排序找到它们各自的中位数
  - 对于上一步中找到的所有中位数，调用 BFPRT 算法求出它们的中位数，作为主元；
- 以主元为分界点，把小于主元的放在左边，大于主元的放在右边；
- 判断主元的位置与 k 的大小，有选择的对左边或右边递归

> 为什么分为5组？
>
> 首先，偶数排除，因为对于奇数来说，中位数更容易计算。
>
> 如果选用3，则有![image-20231113202926252](%E5%9C%BA%E6%99%AF%E7%AE%97%E6%B3%95%E9%A2%98.assets/image-20231113202926252.png)其操作元素个数还是 `n` 。
>
> 如果选取7，9或者更大，在插入排序时耗时增加，常数 `c` 会很大，有些得不偿失。
>
> 综合选择，选定了分为5组。

**复杂度分析**

- 最坏时间复杂度为 O(n) 

> ![image-20231113202716164](%E5%9C%BA%E6%99%AF%E7%AE%97%E6%B3%95%E9%A2%98.assets/image-20231113202716164.png)

#### 总结

求topk问题其实并不难，主要有以下几个思路：

- **整体排序**：O(nlogn)
- **局部排序**：只冒泡排序前k个最大值，O(n*k)
- **利用堆**：O(nlogk)
- **计数或桶排序**：计数排序用于前k个最值，时间复杂度为O(n + m)，其中 m 表示数据范围；桶排序用于最高频k个，时间复杂度为O(n)； **但这两者都要求输入数据必须是有确定范围的整数** 
- **优化：快速选择（quickselect）算法**：平均O(n)，最坏O(n<sup>2</sup>)
- **继续优化：中位数的中位数（bfprt）算法**：最坏O(n)

这里简单说一下后两种方案：

- 首先，需要知道什么是快排，快排的过程简单的说只有三步：首先从序列中选取一个数作为基准数；将比这个数大的数全部放到它的右边，把小于或者等于它的数全部放到它的左边 （一次快排 `partition`）；然后分别对基准的左右两边重复以上的操作，直到数组完全排序
- 快速选择（quickselect）算法：快排会把所有的数据进行排序，而我们仅仅想要的是第 k 个最大值，所以 `quickselect` 对快排进行来优化：在每执行一次快排（`partition`）的时候，比较基准值位置是否在 `n-k` （第k大=第n-k小，n为数组长度）位置上，如果小于 `n-k` ，则第 k 个最大值在基准值的右边，我们只需递归快排基准值右边的子序列即可；如果大于 `n-k` ，则第 k 个最大值在基准值的做边，我们只需递归快排基准值左边的子序列即可；如果等于  `n-k` ，则第 k 个最大值就是基准值，平均时间复杂度O(n)，最坏情况时间复杂度为O(n<sup>2</sup>)，在最坏情况下，时间复杂度还是很高的
- 中位数的中位数（bfprt）算法：该算法的思想是修改快速选择（`quickselect`）算法的主元选取方法，提高算法在最坏情况下的时间复杂度

### Top K 高频词

> 有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。

【解法】

> 由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用**分治策略**，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。

首先遍历大文件，对遍历到的每个词 x，执行 `hash(x) % 5000` ，将结果为 i 的词存放到文件 ai 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。

接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 `map.put(x, 1)` ；若存在，则执行 `map.put(x, map.get(x)+1)` ，将该词频数加 1。

上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个**小顶堆**来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个**小顶堆**，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为**小顶堆**，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。

### 访问次数最多的单个ip

> 现有海量日志数据保存在一个超大文件中，该文件无法直接读入内存，要求从中提取某天访问百度次数最多的那个 IP。

【解法】

> 这里只需要找出出现次数最多的 IP，可以不必使用堆，直接用一个变量 max 即可。

这道题只关心某一天访问百度最多的 IP，因此，可以首先对文件进行一次遍历，把这一天访问百度 IP 的相关信息记录到一个单独的大文件中。

接下来采用的方法与TOP K高频问题一致，大致就是先对 IP 进行哈希映射，接着使用 HashMap 统计重复 IP 的次数，最后计算出重复次数最多的 IP。

## 分治思想

### 如何从大量的URL中找出相同的URL

> 给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL

【解法一】分治思想

> 把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。

首先遍历文件 a，对遍历到的 URL 求 `hash(URL) % 1000` ，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, ..., a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, ..., b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, ..., a999 对应 b999，**不对应的小文件不可能有相同的 URL**。

那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了，利用hashset单独遍历每个小文件ai\bi( `i∈[0,999]` )，看在 HashSet 集合中是否存在。

【解法二】使用字典树

### 从大量数据中找到中位数

> 从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 `(N+1)/2` 个数；当样本数为偶数时，中位数为 第 `N/2` 个数与第 `1+N/2` 个数的均值。

如果这道题没有内存大小限制，则可以把所有数读到内存中排序后找出中位数。但是最好的排序算法的时间复杂度都为 `O(NlogN)` 。

【解法一】大小顶堆

维护两个堆，一个大顶堆，一个小顶堆：

- 大顶堆中最大的数**小于等于**小顶堆中最小的数；
- 保证这两个堆中的元素个数的差不超过 1。

若数据总数为**偶数**，当这两个堆建好之后，**中位数就是这两个堆顶元素的平均值**。当数据总数为**奇数**时，根据两个堆的大小，**中位数一定在数据多的堆的堆顶**。

这种方法需要把所有数据都加载到内存中。当数据量很大时，就不能这样了，因此，这种方法**适用于数据量较小的情况**。

【解法二】分治思想

1. step1大文件切分小文件：顺序读取这 5 亿个数字，对于读取到的数字 num，如果它对应的二进制中最高位为 1，则把这个数字写到 文件f1 中，否则写入 文件f0 中。通过这一步，可以把这 5 亿个数划分为两部分，而且 f0 中的数都大于 f1 中的数【最高位为符号位，为1的为负数更小】。
2. step2分析确定目标在哪个小文件：划分之后，可以非常容易地知道中位数是在 f0 还是 f1 中。假设 f1 中有 1 亿个数，那么中位数一定在 f0 中，且是在 f0 中，从小到大排列的第 1.5 亿个数与它后面的一个数的平均值。
3. 迭代重复：对于 f0 可以用次高位的二进制继续将文件一分为二，如此划分下去，直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数。
4. 当数据总数为偶数，如果划分后两个文件中的数据有相同个数，那么中位数就是数据较小的文件中的最大值与数据较大的文件中的最小值的平均值。

## 数据重复问题

### 大量数据中找到不重复的数

> 在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。

【解法一】分治法

先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。

【解法二】位图法

假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 2<sup>32</sup>。

**那么对于这道题**，我们用 2 个 bit 来表示各个数字的状态：

- 00 表示这个数字没出现过；
- 01 表示这个数字出现过一次（即为题目所找的不重复整数）；
- 10 表示这个数字出现了多次。

那么这 2<sup>32</sup> 个整数，总共所需内存为 2<sup>32</sup>*2b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作：

遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。

> 【补充】
>
> **位图**，就是用一个或多个 bit 来标记某个元素对应的值，而键就是该元素。采用位作为单位来存储数据，可以大大节省存储空间。
>
> 假设我们要对 `[0,7]` 中的 5 个元素 (6, 4, 2, 1, 5) 进行排序，可以采用位图法。0~7 范围总共有 8 个数，只需要 8bit，即 1 个字节。首先将每个位都置 0：
>
> ```
> 0 0 0 0 0 0 0 0
> ```
>
> 然后遍历 5 个元素，首先遇到 6，那么将下标为 6 的位的 0 置为 1；接着遇到 4，把下标为 4 的位 的 0 置为 1：
>
> ```
> 0 0 0 0 1 0 1 0
> ```
>
> 依次遍历，结束后，位数组是这样的：
>
> ```
> 0 1 1 0 1 1 1 0
> ```
>
> 每个为 1 的位，它的下标都表示了一个数。

### 统计不同数字的个数

> 已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。

【解法】

这道题本质还是求解**数据重复**的问题，对于这类问题，一般首先考虑位图法。

对于本题，8 位电话号码可以表示的号码个数为 108 个，即 1 亿个。我们每个号码用一个 bit 来表示，则总共需要 1 亿个 bit，内存占用约 12M。

1. 申请一个位图数组，长度为 1 亿，初始化为 0。
2. 遍历所有电话号码，把号码对应的位图中的位置置为 1。
3. 遍历完成后，如果 bit 为 1，则表示这个电话号码在文件中存在，否则不存在。
4. bit 值为 1 的数量即为 不同电话号码的个数。

## 字符串查找

### 字符串频率问题

> 搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询串的长度不超过 255 字节。
>
> 假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）

【解法一】分治加大小堆

划分为多个小文件，保证单个小文件中的字符串能被直接加载到内存中处理，然后求出每个文件中出现次数最多的 10 个字符串；最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串。

【解法二】前缀树法

当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。

在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。

最后依然使用小顶堆来对字符串的出现次数进行排序。