@autoHeader: 2.1.1.1.1.1
<p align="right">update time : {docsify-updated}</p>

## 订阅机制

Redis 的 SUBSCRIBE 命令可以让客户端订阅任意数量的频道， 每当有新信息发送到被订阅的频道时， 信息就会被发送给所有订阅指定频道的客户端。

![image-20230815132246819](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230815132246819.png)

### 基于频道Channel的订阅

#### 使用命令

**发布**

发布者发布消息的命令是 publish,用法是 publish channel message，如向 channel1.1说一声hi

```bash
127.0.0.1:6379> publish channel:1 hi
(integer) 1
```

这样消息就发出去了。返回值表示接收这条消息的订阅者数量。发出去的消息不会被持久化，也就是有客户端订阅channel:1后只能接收到后续发布到该频道的消息，之前的就接收不到了。

**订阅**

订阅频道的命令是 subscribe，可以同时订阅多个频道，用法是 subscribe channel1 [channel2 ...]

```shell
127.0.0.1:6379> subscribe channel:1
Reading messages... (press Ctrl-C to quit)
1) "subscribe" // 消息类型
2) "channel:1" // 频道
3) "hi" // 消息内容
```

进入订阅状态后客户端可能收到3种类型的回复。每种类型的回复都包含3个值，第一个值是消息的类型，根据消类型的不同，第二个和第三个参数的含义不同：。

消息类型的取值可能是以下3个:

- subscribe。表示订阅成功的反馈信息。第二个值是订阅成功的频道名称，第三个是当前客户端订阅的频道数量。
- message。表示接收到的消息，第二个值表示产生消息的频道名称，第三个值是消息的内容。
- unsubscribe。表示成功取消订阅某个频道。第二个值是对应的频道名称，第三个值是当前客户端订阅的频道数量，当此值为0时客户端会退出订阅状态，之后就可以执行其他非"发布/订阅"模式的命令了。

#### 内部实现

底层是通过字典（图中的pubsub_channels）实现的，这个字典就用于保存订阅频道的信息：字典的键为正在被订阅的频道， 而字典的值则是一个链表， 链表中保存了所有订阅这个频道的客户端。

 ![image-20230815133111566](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230815133111566.png)

- 当客户端调用 SUBSCRIBE 命令时， 程序就将客户端和要订阅的频道在 pubsub_channels 字典中关联起来。
- 当调用 `PUBLISH channel message` 命令， 程序首先根据 channel 定位到字典的键， 然后将信息发送给字典值链表中的所有客户端。
- 使用 UNSUBSCRIBE 命令可以退订指定的频道， 这个命令执行的是订阅的反操作： 它从 pubsub_channels 字典的给定频道（键）中， 删除关于当前客户端的信息， 这样被退订频道的信息就不会再发送给这个客户端。

### 基于模式Pattern的订阅

如果有某个/某些模式和这个频道匹配的话，那么所有订阅这个/这些频道的客户端也同样会收到信息。

#### 使用命令

通配符中?表示1个占位符，*表示任意个占位符(包括0)，?*表示1个以上占位符。

**发布 publish**

```sh
127.0.0.1:6379> publish c m1
(integer) 0
```

**订阅psubscribe**

```shell
127.0.0.1:6379> psubscribe c? b* d?*
Reading messages... (press Ctrl-C to quit)
1) "psubscribe"
2) "c?"
3) (integer) 1
1) "psubscribe"
2) "b*"
3) (integer) 2
1) "psubscribe"
```

1. 使用psubscribe命令可以重复订阅同一个频道，如客户端执行了`psubscribe c? c?*`。这时向c1发布消息客户端会接受到两条消息，而同时publish命令的返回值是2而不是1。同样的，如果有另一个客户端执行了`subscribe c1` 和`psubscribe c?*`的话，向c1发送一条消息该客户顿也会受到两条消息(但是是两种类型:message和pmessage)，同时publish命令也返回2.
2. punsubscribe命令可以退订指定的规则，用法是: `punsubscribe [pattern [pattern ...]]`,如果没有参数则会退订所有规则。
3. 使用punsubscribe只能退订通过psubscribe命令订阅的规则，不会影响直接通过subscribe命令订阅的频道；同样unsubscribe命令也不会影响通过psubscribe命令订阅的规则。另外需要**注意punsubscribe命令退订某个规则时不会将其中的通配符展开，而是进行严格的字符串匹配**，所以`punsubscribe *` 无法退订`c*`规则，而是必须使用`punsubscribe c*`才可以退订。

#### 内部实现

底层是List<pubsubPattern>。

```c
typedef struct pubsubPattern {
    redisClient *client;
    robj *pattern;
} pubsubPattern;
```

lient 属性保存着订阅模式的客户端，而 pattern 属性则保存着被订阅的模式。

- 每当调用 PSUBSCRIBE 命令订阅一个模式时， 程序就创建一个包含客户端信息和被订阅模式的 pubsubPattern 结构， 并将该结构添加到 redisServer.pubsub_patterns 链表中。

![image-20230815133333311](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230815133333311.png)

- 发送信息到模式的工作也是由 PUBLISH 命令进行的, 通过遍历整个 pubsub_patterns 链表，程序可以检查所有正在被订阅的模式，以及订阅这些模式的客户端。通过匹配模式获得Channels，然后再把消息发给客户端。
- 退订： 程序会删除 redisServer.pubsub_patterns 链表中， 所有和被退订模式相关联的 pubsubPattern 结构， 这样客户端就不会再收到和模式相匹配的频道发来的信息。

## 持久化机制

### 持久化的必要性

Redis是个基于内存的数据库。那服务一旦宕机，内存中的数据将全部丢失。

通常的解决方案是从后端数据库恢复这些数据，但后端数据库有性能瓶颈，如果是大数据量的恢复，1、会对数据库带来巨大的压力，2、数据库的性能不如Redis。导致程序响应慢。

所以对Redis来说，实现数据的持久化，避免从后端数据库中恢复数据，是至关重要的。

### RDB机制

#### 实现方式

RDB是Redis默认的持久化方式。

RDB持久化是通过**快照**的方式，即在指定的时间间隔内将内存中的数据集快照写入磁盘。在创建快照之后，用户可以备份该快照，可以将快照复制到其他服务器以创建相同数据的服务器副本，或者在重启服务器后恢复数据。

RDB持久化会生成RDB文件，该文件是一个**压缩**过的**二进制文件**，可以通过该文件还原快照时的数据库状态，即生成该RDB文件时的服务器数据。

#### 触发时机

触发rdb持久化的方式有2种，分别是**手动触发**和**自动触发**。

##### 手动触发

手动触发分别对应save和bgsave命令。

- save命令：阻塞当前Redis服务器，直到RDB过程完成为止，对于内存 比较大的实例会造成长时间阻塞，线上环境不建议使用。

- bgsave命令：Redis进程执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。阻塞只发生在fork子进程的阶段，一般时间很短，具体流程：
  - redis客户端执行bgsave命令或者自动触发bgsave命令；
  - 主进程判断当前是否已经存在正在执行的子进程，如果存在，那么主进程直接返回；
  - 如果不存在正在执行的子进程，那么就fork一个新的子进程进行持久化数据，fork过程是阻塞的，fork操作完成后主进程即可执行其他操作；
  - 子进程先将数据写入到临时的rdb文件中，待快照数据写入完成后再原子替换旧的rdb文件；
  - 同时发送信号给主进程，通知主进程rdb持久化完成，主进程更新相关的统计信息（info Persitence下的rdb_*相关选项）

![image-20230814212103665](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230814212103665.png)

##### 自动触发

在以下4种情况时会自动触发：

1. redis.conf中配置`save m n`，即在m秒内有n次修改时，自动触发bgsave生成rdb文件；
2. 主从复制时，从节点要从主节点进行全量复制时也会触发bgsave操作，生成当时的快照发送到从节点；
3. 执行debug reload命令重新加载redis时也会触发bgsave操作；
4. 默认情况下执行shutdown命令时，如果没有开启aof持久化，那么也会触发bgsave操作；

#### 配置参数

redis.conf中的配置

**1 快照周期**

内存快照虽然可以通过技术人员手动执行SAVE或BGSAVE命令来进行，但生产环境下多数情况都会设置其周期性执行条件，其参数根据自己的实际请求压力进行设置调整。

```shell
# 周期性执行条件的设置格式为
save <seconds> <changes>

# 默认的设置为：
save 900 1 # 如果900秒内有1条Key信息发生变化，则进行快照；
save 300 10
save 60 10000

# 以下设置方式为关闭RDB快照功能
save ""
```

*2 文件名称**

RDB文件在磁盘上的名称

```she
# 文件名称
dbfilename dump.rdb
```

**3 存储路径**

RDB文件的存储路径。默认设置为“./”，也就是Redis服务的主目录

```shell
# 文件保存路径
dir /home/work/app/redis/data/
```

**4 持久化出错**

提到的在快照进行过程中，主进程照样可以接受客户端的任何写操作的特性，是指在快照操作正常的情况下。

如果快照操作出现异常（例如操作系统用户权限不够、磁盘空间写满等等）时，Redis就会禁止写操作。这个特性的主要目的是使运维人员在第一时间就发现Redis的运行错误，并进行解决。一些特定的场景下，您可能需要对这个特性进行配置，这时就可以调整这个参数项。该参数项默认情况下值为yes，如果要关闭这个特性，指定即使出现快照错误Redis一样允许写操作，则可以将该值更改为no。

```shell
# 如果持久化出错，主进程是否停止写入
stop-writes-on-bgsave-error yes
```

**5 是否压缩**

该属性将在字符串类型的数据被快照到磁盘文件时，启用LZF压缩算法。

Redis官方的建议是请保持该选项设置为yes，因为“it’s almost always a win”

```shell
# 是否压缩
rdbcompression yes
```

**6 导入检查**

RDB快照功能的version 5 版本开始，一个64位的CRC冗余校验编码会被放置在RDB文件的末尾，以便对整个RDB文件的完整性进行验证。这个功能大概会多损失10%左右的性能，但获得了更高的数据可靠性。

```shell
# 导入时是否检查
rdbchecksum yes
```



#### 优缺点

**优点**

- RDB文件是某个时间节点的快照，默认使用LZF算法进行压缩，压缩后的文件体积远远小于内存大小，适用于备份、全量复制等场景；
- Redis加载RDB文件恢复数据要远远快于AOF方式；

**缺点**

- RDB方式实时性不够，无法做到秒级的持久化；
- 每次调用bgsave都需要fork子进程，fork子进程属于重量级操作，频繁执行成本较高；
- RDB文件是二进制的，没有可读性，AOF文件在了解其结构的情况下可以手动修改或者补全；
- 版本兼容RDB文件问题；

#### 实战问题

##### 快照过程中的数据一致性

**问题描述**

由于生产环境中我们为Redis开辟的内存区域都比较大（例如6GB），那么将内存中的数据同步到硬盘的过程可能就会持续比较长的时间，而实际情况是这段时间Redis服务一般都会收到数据写操作请求。那么如何保证数据一致性呢？

**解答**

RDB中的核心思路是Copy-on-Write，来保证在进行快照操作的这段时间，需要压缩写入磁盘上的数据在内存中不会发生变化。

在正常的快照操作中，一方面Redis主进程会fork一个新的快照进程专门来做这个事情，这样保证了Redis服务不会停止对客户端包括写请求在内的任何响应。另一方面这段时间发生的数据变化会以副本的方式存放在另一个新的内存区域，待快照操作结束后才会同步到原来的内存区域。

> 比如：如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。
>
> ![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/redis-x-aof-42.jpg)

##### 快照过程中服务崩溃

**问题描述**

在进行快照操作的这段时间，如果发生服务崩溃怎么办？

**解答**

在没有将数据全部写入到磁盘前，这次快照操作都不算成功。

如果出现了服务崩溃的情况，将以上一次完整的RDB快照文件作为恢复内存数据的参考。也就是说，在快照操作过程中不能影响上一次的备份数据。

Redis服务会在磁盘上创建一个临时文件进行数据操作，待操作成功后才会用这个临时文件替换掉上一次的备份。

##### 快照频率

**问题描述**

可以一秒做一次快照吗？如何尽可能避免丢失数据。

**解答**

快照的间隔时间变得很短，即使某一时刻发生宕机了，因为上一时刻快照刚执行，丢失的数据也不会太多。但是，如果频繁地执行全量快照，也会带来两方面的开销：

一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。

另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了。

为了避免丢失数据，可以使用增量快照，采用RDB和AOF的混合模式。

### AOF机制

#### 实现方式

AOF持久化会把被执行的写命令写到AOF文件的末尾，记录数据的变化。AOF日志采用写后日志，即先写内存，后写日志。

Redis先执行命令，把数据写入内存，然后才记录日志。日志里记录的是Redis收到的每一条命令，这些命令是以文本形式保存。

> 其实大多数的数据库采用的是写前日志（WAL），例如MySQL，通过写前日志和两阶段提交，实现数据和逻辑的一致性。

**Redis为什么采用写后日志**？

Redis要求高性能，采用写后日志有两方面好处：

1. 避免额外的检查开销：Redis 在向 AOF 里面记录日志的时候，并不会先去对这些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错。
2. 不会阻塞当前的写操作

但这种方式存在潜在风险：

1. 如果命令执行完成，写日志之前宕机了，会丢失数据。
2. 主线程写磁盘压力大，导致写盘慢，阻塞后续操作。

#### 步骤及触发时机

##### AOF文件结构

执行两个写操作

```shell
127.0.0.1:6379> set s1 hello
OK
127.0.0.1:6379> set s2 world
OK
```

查看AOF中的记录：

```shell
*3
$3
set
$2
s1
$5
hello
*3
$3
set
$2
s2
$5
world
```

该命令格式为Redis的序列化协议（RESP）:

其中`*3`代表这个命令有三个参数，`$3`表示该命令中的参数a length长度为3

##### AOF步骤

AOF日志记录Redis的每个写命令，步骤分为：

1. 命令追加（append）：服务器每执行一个写命令，都会把该命令以协议格式先追加到AOF文件的内存缓存区的末尾，而不是直接写入文件，避免每次有命令都直接写入硬盘，减少硬盘IO次数

2. 文件写入（write）：将AOF缓存区的内容写入IO缓冲区。

3. 文件同步（fsync）：将IO缓冲区的内容同步到磁盘上的AOF文件。

##### AOF执行时机

其中，文件写入及同步的时机，Redis中有三种策略：

> 为了提高文件写入效率，在现代操作系统中，当用户调用write函数，将一些数据写入文件时，操作系统通常会将数据暂存到一个内存缓冲区里，当缓冲区的空间被填满或超过了指定时限后，才真正将缓冲区的数据写入到磁盘里。 这样的操作虽然提高了效率，但也为数据写入带来了安全问题：如果计算机停机，内存缓冲区中的数据会丢失。为此，系统提供了fsync、fdatasync同步函数，可以强制操作系统立刻将缓冲区中的数据写入到硬盘里，从而确保写入数据的安全性。

| 配置项   | 写回时机                                                     | 优点                       | 缺点                             |
| -------- | ------------------------------------------------------------ | -------------------------- | -------------------------------- |
| Always   | 每个写append命令执行完，立马同步地将日志写回磁盘。<br>一旦aof缓冲区有数据时，就调用fsync命令强制往aof文件写数据 | 可靠性高，数据基本上不丢失 | 每个命令都同步落盘，性能影响较大 |
| Everysec | 每个append命令执行完，只是先把日志写到AOF缓冲区，每隔一秒把缓冲区中的内容写入磁盘（有另外一个线程负责）<br>一旦aof缓冲区有数据时，就调用write命令往io缓冲区写数据，同时每秒调用一次fsync命令强制将io缓冲区的数据写入到aof文件 | 性能适中                   | 宕机时一般情况下不超过2秒的数据  |
| No       | 每个写append命令执行完，只是先把日志写到AOF内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘<br>一旦aof缓冲区有数据时，就调用wirte命令往io缓冲区写数据。由操作系统定期或当io缓冲区填满时，自动将io缓冲区的数据写入到aof文件 | 性能好                     | 宕机时丢失数据较多               |

**Always**

在这种模式下，每次执行完一个命令之后， write和 fsync 都会被执行。

另外，因为 fsync 是由 Redis 主进程执行的，所以在 fsync 执行期间，主进程会被阻塞，不能接受命令请求。

**Everysec**

在这种模式中， SAVE 原则上每隔一秒钟就会执行一次， 因为 SAVE 操作是由后台子线程调用的， 所以它不会引起服务器主进程阻塞。

注意， 在上一句的说明里面使用了词语“原则上”， 在实际运行中， 程序在这种模式下对 `fsync` 的调用并不是每秒一次， 它和调用时 Redis 所处的状态有关：

![image-20230815082536396](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230815082536396.png)

**No**

在这种模式下， 每次调用 append 函数， WRITE 都会被执行， 但 fsync 会被略过。

在这种模式下， SAVE 只会在以下任意一种情况中被执行：

- Redis 被关闭
- AOF 功能被关闭
- 系统的写缓存被刷新（可能是缓存已经被写满，或者定期保存操作被执行）

这三种情况下的 SAVE 操作都会引起 Redis 主进程阻塞。

#### 配置参数

默认情况下，Redis是没有开启AOF的，可以通过配置redis.conf文件来开启AOF持久化。

```shell
# appendonly参数开启AOF持久化
appendonly no

# AOF持久化的文件名，默认是appendonly.aof
appendfilename "appendonly.aof"

# AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的
dir ./

# 同步策略
# appendfsync always
appendfsync everysec
# appendfsync no

# aof重写期间是否同步
no-appendfsync-on-rewrite no

# 重写触发配置
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# 加载aof出错如何处理
aof-load-truncated yes

# 文件重写策略
aof-rewrite-incremental-fsync yes
```

#### 优缺点

优点：

- 数据更完整，安全性更高，秒级数据丢失（取决fsync策略，如果是everysec，最多丢失1秒的数据）
- AOF文件是一个只进行追加的日志文件，且写入操作是以Redis协议的格式保存的，内容是可读的，适合误删紧急恢复

缺点：

- 对于相同的数据集，AOF文件的体积要大于RDB文件，数据恢复也会比较慢
- 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB。 不过在一般情况下， 每秒 fsync 的性能依然非常高

#### AOF重写

AOF会记录每个写命令到AOF文件，随着时间越来越长，AOF文件会变得越来越大。如果不加以控制，会对Redis服务器，甚至对操作系统造成影响，而且AOF文件越大，数据恢复也越慢。

为了解决AOF文件体积膨胀的问题，Redis提供AOF文件重写机制来对AOF文件进行“瘦身”，新旧两个AOF文件保存的数据一致，但新AOF文件没有冗余命令。

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/redis-x-aof-1.jpg)

**AOF文件重写并不需要对现有的AOF文件进行任何读取、分享和写入操作，而是通过读取服务器当前的数据库状态来实现的**

##### 手动触发

手动触发执行`bgrewriteaof`命令，该命令的执行跟`bgsave`触发快照时类似的，都是先`fork`一个子进程做具体的工作：

```shell
127.0.0.1:6379> bgrewriteaof
Background append only file rewriting started
```

##### 自动触发

自动触发会根据`auto-aof-rewrite-percentage`和`auto-aof-rewrite-min-size 64mb`配置来自动执行`bgrewriteaof`命令：

```shell
# 表示当AOF文件的体积大于64MB，且AOF文件的体积比上一次重写后的体积大了一倍（100%）时，会执行`bgrewriteaof`命令
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
```

##### 重写流程

![image-20230815084026661](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230815084026661.png)

- 重写会有大量的写入操作，所以服务器进程会`fork`一个子进程来创建一个新的AOF文件
- 在重写期间，服务器进程继续处理命令请求，如果有写入的命令，追加到`aof_buf`的同时，还会追加到`aof_rewrite_buf`AOF重写缓冲区
- 当子进程完成重写之后，会给父进程一个信号，然后父进程会把AOF重写缓冲区的内容写进新的AOF临时文件中，再对新的AOF文件改名完成替换，这样可以保证新的AOF文件与当前数据库数据的一致性

**子进程复制内存数据**

**AOF文件重写不对现有的AOF文件操作，而是复制一份当前内存中的数据。**

采用操作系统提供的写时复制（copy on write）机制，就是为了避免一次性拷贝大量内存数据给子进程造成阻塞。

fork子进程时，子进程时会拷贝父进程的页表，而不会拷贝物理内存。这个拷贝会消耗大量cpu资源，并且拷贝完成前会阻塞主线程，阻塞时间取决于内存中的数据量，数据量越大，则内存页表越大。拷贝完成后，父子进程使用相同的内存地址空间。

主进程是可以有数据写入的，操作系统会创建这个页面的副本（页c的副本），即拷贝当前页的物理数据，将其映射到主进程中，而子进程还是使用原来的的页c。

![image-20230815085019356](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230815085019356.png)

##### 其他问题

**1 在重写日志整个过程时，主线程有哪些地方会被阻塞**

1. fork子进程时，需要拷贝虚拟页表，会对主线程阻塞。
2. 主进程有bigkey写入时，操作系统会创建页面的副本，并拷贝原有的数据，会对主线程阻塞。
3. 子进程重写日志完成后，主进程追加aof重写缓冲区时可能会对主线程阻塞。

**2 为什么AOF重写不复用原AOF日志**？

1. 父子进程写同一个文件会产生竞争问题，影响父进程的性能。
2. 如果AOF重写过程中失败了，相当于污染了原本的AOF文件，无法做恢复数据使用。

**3 重写日志时，有新数据写入情况**

在fork出子进程时的拷贝，以及在重写时，如果有新数据写入，主线程就会将命令记录到两个aof日志内存缓冲区中。如果AOF写回策略配置的是always，则直接将命令写回旧的日志文件，并且保存一份命令至AOF重写缓冲区，这些操作对新的日志文件是不存在影响的。

而在bgrewriteaof子进程完成会日志文件的重写操作后，会提示主线程已经完成重写操作，主线程会将AOF重写缓冲中的命令追加到新的日志文件后面。这时候在高并发的情况下，AOF重写缓冲区积累可能会很大，这样就会造成阻塞，Redis后来通过Linux管道技术让aof重写期间就能同时进行回放，这样aof重写结束后只需回放少量剩余的数据即可。

最后通过修改文件名的方式，保证文件切换的原子性。

**4 重写过程宕机**

在AOF重写日志期间发生宕机的话，因为日志文件还没切换，所以恢复数据时，用的还是旧的日志文件。

#### 如何选择RDB和AOF

1、如果是数据不那么敏感，且可以从其他地方重新生成补回的，那么可以关闭持久化

2、如果是数据比较重要，不想再从其他地方获取，且可以承受数分钟的数据丢失，比如缓存等，那么可以只使用RDB

3、如果是用做内存数据库，要使用Redis的持久化，建议是RDB和AOF都开启，或者定期执行`bgsave`做快照备份，RDB方式更适合做数据的备份，AOF可以保证数据的不丢失。

### AOF和RDB混合方式(4.0)

Redis 4.0 中提出了一个**混合使用 AOF 日志和内存快照**的方法：内存快照RDB以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。

> 这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF 日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。

如下图所示，T1 和 T2 时刻的修改，用 AOF 日志记录，等到第二次做全量快照时，就可以清空 AOF 日志，因为此时的修改都已经记录到快照中了，恢复时就不再用日志了。

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/redis-x-rdb-4-20230815083307446.jpg)

在引入了混合持久化之后，使用 AOF 重建数据集时，会通过文件开头是否为“REDIS”来判断是否为混合持久化。

### 数据恢复

想要从文件中恢复数据，只需要重新启动Redis即可。

![image-20230909150056498](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230909150056498.png)

- redis重启时判断是否开启aof，如果开启了aof，那么就优先加载aof文件；**在AOF文件中判断是否开启了混合模式，如果包含REDIS开头，则证明开启了持久化，则先载入RDB，再载入AOF；否则直接载入AOF**
- 如果aof存在，那么就去加载aof文件，加载成功的话redis重启成功，如果aof文件加载失败，那么会打印日志表示启动失败，此时可以去修复aof文件后重新启动；
- 若aof文件不存在，那么redis就会转而去加载rdb文件，如果rdb文件不存在，redis直接启动成功；
- 如果rdb文件存在就会去加载rdb文件恢复数据，如加载失败则打印日志提示启动失败，如加载成功，那么redis重启成功，且使用rdb文件恢复数据；

**为什么会优先加载AOF**

因为AOF保存的数据更完整，AOF基本上最多损失2s的数据。

### 实际经验

RDB的快照、AOF的重写都需要fork，这是一个重量级操作，会对Redis造成阻塞。因此为了不影响Redis主进程响应，我们需要尽可能降低阻塞：

- 降低fork的频率，比如可以手动来触发RDB生成快照、与AOF重写；
- 控制Redis最大使用内存，防止fork耗时过长；
- 使用更牛逼的硬件；
- 合理配置Linux的内存分配策略，避免因为物理内存不足导致fork失败。

有什么实践经验：

- 如果Redis中的数据并不是特别敏感或者可以通过其它方式重写生成数据，可以关闭持久化，如果丢失数据可以通过其它途径补回；
- 自己制定策略定期检查Redis的情况，然后可以手动触发备份、重写数据；
- 单机如果部署多个实例，要防止多个机器同时运行持久化、重写操作，防止出现内存、CPU、IO资源竞争，让持久化变为串行；
- 可以加入主从机器，利用一台从机器进行备份处理，其它机器正常响应客户端的命令；
- RDB持久化与AOF持久化可以同时存在，配合使用。

## 事务机制

Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。

总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。

### 使用事务

#### 使用命令

- MULTI ：开启事务，redis会将后续的命令逐个放入队列中，然后使用EXEC命令来原子化执行这个命令系列。
- EXEC：执行事务中的所有操作命令。
- DISCARD：取消事务，放弃执行事务块中的所有命令。
- WATCH：监视一个或多个key,如果事务在执行前，这个key(或多个key)被其他命令修改，则事务被中断，不会执行事务中的任何命令。
- UNWATCH：取消WATCH对所有key的监视。

#### 错误处理

| 错误描述                                                     | 错误处理                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 事务在执行 `EXEC` 之前，命令可能会产生语法错误（参数数量错误，参数名错误等等），或者其他更严重的错误，比如内存不足（如果服务器使用 `maxmemory` 设置了最大内存限制的话） | 服务器会对命令入队失败的情况进行记录，并在客户端调用 `EXEC` 命令时，拒绝执行并自动放弃这个事务 |
| 命令可能在 `EXEC` 调用之后失败，如运行时类型错误等           | 没有特殊处理：事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。 |

### watch命令

#### watch命令做了什么

WATCH 命令监控某个key，事务exec时，会跟原值做比较，一旦发现它被修改过，则拒绝执行命令，并且会返回 `nil` 给客户端，表示事务已经失败。

watch命令为Redis事务提供乐观锁。

> 举个例子， 假设我们需要原子性地为某个值进行增 1 操作。
>
> 首先我们可能会这样做：
>
> ```bash
> val = GET mykey
> val = val + 1
> SET mykey $val
> ```
>
> 上面的这个实现在只有一个客户端的时候可以执行得很好。 但是， 当多个客户端同时对同一个键进行这样的操作时， 就会产生竞争条件。举个例子， 如果客户端 A 和 B 都读取了键原来的值， 比如 10 ， 那么两个客户端都会将键的值设为 11 ， 但正确的结果应该是 12 才对。
>
> 有了 WATCH ，我们就可以轻松地解决这类问题了：
>
> ```bash
> WATCH mykey
> val = GET mykey
> val = val + 1
> MULTI
> SET mykey $val
> EXEC
> ```
>
> 使用上面的代码， 如果在 WATCH 执行之后， EXEC 执行之前， 有其他客户端修改了 mykey 的值， 那么当前客户端的事务就会失败。 程序需要做的， 就是不断重试这个操作， 直到没有发生碰撞为止。

举例：

在事务开始前用WATCH监控k1，之后修改k1为11，说明事务开始前k1值被改变，MULTI开始事务，修改k1值为12，k2为22，执行EXEC，发回nil，说明事务回滚；查看下k1、k2的值都没有被事务中的命令所改变。

```bash
127.0.0.1:6379> set k1 v1
OK
127.0.0.1:6379> set k2 v2
OK
127.0.0.1:6379> WATCH k1
OK
127.0.0.1:6379> set k1 11
OK
127.0.0.1:6379> MULTI
OK
127.0.0.1:6379> set k1 12
QUEUED
127.0.0.1:6379> set k2 22
QUEUED
127.0.0.1:6379> EXEC
(nil)
127.0.0.1:6379> get k1
"11"
127.0.0.1:6379> get k2
"v2"
```

#### 如何实现

Redis使用WATCH命令来决定事务是继续执行还是回滚，那就需要在MULTI之前使用WATCH来监控某些键值对，然后使用MULTI命令来开启事务，执行对数据结构操作的各种命令，此时这些命令入队列。

当使用EXEC执行事务时，首先会比对WATCH所监控的键值对，如果没发生改变，它会执行事务队列中的命令，提交事务；如果发生变化，将不会执行事务中的任何命令，同时事务回滚。当然无论是否回滚，Redis都会取消执行事务前的WATCH命令。

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/db-redis-trans-2.png)

### 执行步骤

redis事务执行是三个阶段：

1. **开启**：以MULTI开始一个事务
2. **入队**：将多个命令入队到事务中，接到这些命令并不会立即执行，而是放到等待执行的事务队列里面
3. **执行**：由EXEC命令触发事务

当一个客户端切换到事务状态之后， 服务器会根据这个客户端发来的不同命令执行不同的操作：

- 如果客户端发送的命令为 EXEC 、 DISCARD 、 WATCH 、 MULTI 四个命令的其中一个， 那么服务器立即执行这个命令。
- 与此相反， 如果客户端发送的命令是 EXEC 、 DISCARD 、 WATCH 、 MULTI 四个命令以外的其他命令， 那么服务器并不立即执行这个命令， 而是将这个命令放入一个事务队列里面， 然后向客户端返回 QUEUED 回复。

> 根据这个步骤看上面错误处理：
>
> - 编译错误在入队时被发现，此时真的Redis命令还未执行，因此值均为改变。
> - 运行错误发送在已经入完队，正在执行命令的过程中，错误之前的语句已经执行过，值的更改已经发生。

### 其他问题

#### Redis事务不支持回滚

主要就是 3个原因：

- 作者认为发生事务回滚的原因大部分都是程序错误导致，这种情况一般发生在开发和测试阶段，而生产环境很少出现。
- 对于逻辑性错误，比如本来应该把一个数加 1，但是程序逻辑写成了加2，那么这种错误也是无法通过事务回滚来进行解决的。
- Redis追求的是简单高效，而传统事务的实现相对比较复杂，这和 Redis的设计思想相违背。

#### Redis事务满足ACID吗

Redis的事务机制可满足原子性、一致性、隔离性，不满足持久性。

- **原子性atomicity**

首先通过上文知道 运行期的错误是不会回滚的，很多文章由此说Redis事务违背原子性的；而官方文档认为是遵从原子性的。

Redis官方文档给的理解是，**Redis的事务是原子性的：所有的命令，要么全部执行，要么全部不执行，但不是完全成功。**

- **一致性consistency**

一致性指的就是事务执行前后的数据符合数据库的定义和要求。这一点 `Redis` 中的事务是符合要求的，上面讲述原子性的时候已经提到，不论是发生语法错误还是运行时错误，错误的命令均不会被执行。

- **隔离性Isolation**

可以使用watch命令满足隔离性：

1. 并发操作在 `EXEC` 命令前执行，隔离性需要通过 `WATCH` 机制保证；
2. 并发操作在 `EXEC` 命令之后，隔离性可以保证。

- **持久性Durability**

**redis事务是不保证持久性的**，这是因为redis持久化策略中不管是RDB还是AOF都是异步执行的，都存在数据丢失的情况。

## 高可用机制

> [!note]主从复制避免单点故障，主从库之间采用的是读写分离的方式；哨兵机制解决了主从复制模式下故障转移，实现主从库自动切换。

### 主从复制机制

主从库之间采用的是**读写分离**的方式。

- 读操作：主库、从库都可以接收；
- 写操作：首先到主库执行，然后，主库将写操作同步给从库。

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/db-redis-copy-1.png)

> 在2.8版本之前只有全量复制，而2.8版本后有全量和增量复制。
>
> 主从复制共有三种模式：**全量复制、基于长连接的命令传播、增量复制**。
>
> 主从服务器第一次同步的时候，就是采用全量复制。
>
> 第一次同步完成后，主从服务器都会维护着一个长连接，主服务器在接收到写操作命令后，就会通过这个连接将写命令传播给从服务器，来保证主从服务器的数据一致性。
>
> 如果遇到网络断开，则使用增量复制续传。

#### 配置

**主从复制的开启，完全是在从节点发起的，不需要在主节点做任何事情**

可以通过 replicaof（Redis 5.0 之前使用 slaveof）命令形成主库和从库的关系。

在从节点开启主从复制，有 3 种方式：

- 配置文件

在从服务器的配置文件中加入 `replicaof <masterip> <masterport>`

- 启动命令

redis-server 启动命令后面加入 `--replicaof <masterip> <masterport>`

- 客户端命令

启动多个 Redis 实例后，直接通过客户端执行命令：`replicaof <masterip> <masterport>`，则该 Redis 实例成为从节点。

假设现在有实例 1（172.16.88.1）、实例 2（172.16.88.2）和实例 3 (172.16.88.3)，在实例 2 和实例 3 上分别执行以下命令，实例 2 和 实例 3 就成为了实例 1 的从库，实例 1 成为 Master。

```text
replicaof 172.16.88.1 6379
```

#### 全量复制

##### 全量复制阶段

![图片](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/ea4f7e86baf2435af3999e5cd38b6a26.png)

- **第一阶段是主从库间建立连接、协商同步的过程**

主要是为全量复制做准备。在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了。

从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync 命令包含了主库的 runID 和复制进度 offset 两个参数。

> runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为“？”。
>
> offset，此时设为 -1，表示第一次复制。

主库收到 psync 命令后，会用 FULLRESYNC 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。

从库收到响应后，会记录下这两个参数。这里有个地方需要注意，FULLRESYNC 响应表示第一次复制采用的全量复制，也就是说，主库会把当前所有的数据都复制给从库。

- **第二阶段，主库将所有数据同步给从库**。

从库收到数据后，在本地完成数据加载。<font color=red>这个过程依赖于内存快照生成的 RDB 文件。</font>

主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。

从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。这是因为从库在通过 replicaof 命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空。

在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。否则，Redis 的服务就被中断了。但是，这些请求中的写操作并没有记录到刚刚生成的 RDB 文件中。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作。

- **第三个阶段，主库会把第二阶段执行过程中新收到的写命令，再发送给从库**

当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步。

##### buffer 

- `replication buffer`

`replication buffer`缓存将要传递给从服务器的命令，在全量复制中，这个缓冲区记录的是 RDB 文件生成后收到的所有写操作。

#### 基于长连接的命令传播

##### 命令传播

主从服务器在完成增量同步后，双方之间就会维护一个 TCP 连接。

![图片](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/03eacec67cc58ff8d5819d0872ddd41e.png)

后续主服务器可以通过这个连接继续将写操作命令传播给从服务器，然后从服务器执行该命令，使得与主服务器的数据库状态相同。

而且这个连接是长连接的，目的是避免频繁的 TCP 连接和断开带来的性能开销。

上面的这个过程被称为**基于长连接的命令传播**，通过这种方式来保证同步后的主从服务器的数据一致性。

##### PING 

> 在命令传播阶段，除了发送写命令，主从节点还维持着心跳机制：PING 和 REPLCONF ACK。

主->从：PING

每隔指定的时间，**主节点会向从节点发送 PING 命令**，这个 PING 命令的作用，主要是为了让从节点进行超时判断。

##### REPLCONF ACK

从->主：REPLCONF ACK

在命令传播阶段，从服务器默认会以每秒一次的频率，向主服务器发送命令：

```text
REPLCONF ACK <replication_offset>
```

其中 replication_offset 是从服务器当前的复制偏移量。

发送 REPLCONF ACK 命令对于主从服务器有三个作用：

1. 检测主从服务器的网络连接状态。
2. 辅助实现 min-slaves 选项。
3. 检测命令丢失, 从节点发送了自身的 slave_replication_offset，主节点会用自己的 master_replication_offset 对比，如果从节点数据缺失，主节点会从 `repl_backlog_buffer`缓冲区中找到并推送缺失的数据。

**注意，offset 和 repl_backlog_buffer 缓冲区，不仅可以用于增量复制，也可以用于处理命令丢失等情形；区别在于前者是在断线重连后进行的，而后者是在主从节点没有断线的情况下进行的。**

#### 增量复制 

在 Redis 2.8 之前，如果主从服务器在命令同步时出现了网络断开又恢复的情况，从服务器就会和主服务器重新进行一次全量复制，很明显这样的开销太大了。

所以，从 Redis 2.8 开始，网络断开又恢复后，从主从服务器会采用**增量复制**的方式继续同步，也就是只会把网络断开期间主服务器接收到的写操作命令，同步给从服务器。

##### 两个buffer

![图片](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/2db4831516b9a8b79f833cf0593c1f12.png)

- `replication buffer`

`replication buffer`缓存将要传递给从服务器的命令，在增量同步中，这个缓冲区填充的是repl_backlog_buffer中找出的增量数据。

- `repl_backlog_buffer`

断开重连增量复制的实现奥秘就是 `repl_backlog_buffer` 缓冲区，不管在什么时候 master 都会将写指令操作记录在 `repl_backlog_buffer` 中，因为内存有限， `repl_backlog_buffer` 是一个定长的环形数组，**如果数组内容满了，就会从头开始覆盖前面的内容**

repl_baklog中会记录Redis处理过的命令日志及offset，包括master当前的offset和slave已经拷贝的offset： `master_repl_offset`记录master节点的位置偏移量， `slave_repl_offset`记录slave已经同步的偏移量：master 收到写操作，偏移量则会增加；从库持续执行同步的写指令后，slave_repl_offset 也在不断增加。

因此：

- 正常情况下，这两个偏移量基本相等。
- 在网络断连阶段，主库可能会收到新的写操作命令，所以 `master_repl_offset`会大于 `slave_repl_offset`。

> repl_backlog示意图如下，slave与master的offset之间存在差异，就是slave需要增量拷贝的数据了。
>
> ![image-20231211192336285](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231211192336285.png)
>
> 随着不断有数据写入，master的offset会逐渐变大，slave也不断拷贝，追赶这master的offset，直到数组被填满：
>
> ![image-20231211192410738](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231211192410738.png)
>
> 此时，如果有新的数据写入，就会覆盖数组中旧的数据了。不过，旧的数据只要是绿色的，说明是已经被同步到slave的数据，即便被覆盖了也没有什么影响。因为未同步的部分仅仅是红色部分的。
>
> 但是如果slave出现了网络阻塞，或者其他情况，导致master的offset远远的超过了slave的offset，如果master继续写入新的数据，其offset就会覆盖旧的数据，直到将slave现在的offset也覆盖掉：
>
> ![image-20231211192458103](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231211192458103.png)
>
> 棕色框中的红色部分，就是尚未同步，但是却已经被覆盖的数据。此时如果slave恢复，需要同步的时候，却发现自己的offset没有了，无法完成增量同步了。在这种情况下，只能做全量同步了。

如果从库断开时间太久，repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量复制，repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量复制的概率。

计算公式：

 ```text
 repl_backlog_buffer = second * write_size_per_second
 ```

 1. **second**：从服务器断开重连主服务器所需的平均时间；
 2. **write_size_per_second**：master 平均每秒产生的命令数据量大小（写命令和数据大小总和）；

 例如，如果主服务器平均每秒产生 1 MB 的写数据，而从服务器断线之后平均要 5 秒才能重新连接上主服务器，那么复制积压缓冲区的大小就不能低于 5 MB。

 为了安全起见，可以将复制积压缓冲区的大小设为`2 * second * write_size_per_second`，这样可以保证绝大部分断线情况都能用部分重同步来处理。

##### 流程

![图片](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/e081b470870daeb763062bb873a4477e.png)

- 从服务器在恢复网络后，从服务器会通过 psync 命令将自己的复制偏移量 slave_repl_offset 发送给主服务器
- 主服务器根据自己的 master_repl_offset 和 slave_repl_offset 之间的差距，然后来决定对从服务器执行哪种同步操作：
  - 如果判断出从服务器要读取的数据还在 repl_backlog_buffer 缓冲区里，那么主服务器将采用**增量同步**的方式；
  - 相反，如果判断出从服务器要读取的数据已经不存在 repl_backlog_buffer 缓冲区里，那么主服务器将采用**全量同步**的方式
- 如果是增量复制，主服务器用 CONTINUE 响应命令告诉从服务器接下来采用增量复制的方式同步数据；主服务器在 repl_backlog_buffer 中找到主从服务器差异的数据后，就会将增量的数据写入到 replication buffer 缓冲区，发送给从服务器
- 如果是全量复制，主服务器回复从节点 `FULLRESYNC <runid> <offset>`，表示要进行全量复制

#### 主-从-从架构优化

主服务器是可以有多个从服务器的，如果从服务器数量非常多，而且都与主服务器进行全量同步的话，就会带来两个问题：

- 由于是通过 bgsave 命令来生成 RDB 文件的，那么主服务器就会忙于使用 fork() 创建子进程，如果主服务器的内存数据非大，在执行 fork() 函数时是会阻塞主线程的，从而使得 Redis 无法正常处理请求；
- 传输 RDB 文件会占用主服务器的网络带宽，会对主服务器响应命令请求产生影响。

因此，可以通过“主 - 从 - 从”模式**将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上**。

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/db-redis-copy-4.jpg)

在部署主从集群的时候，可以手动选择一个从库（比如选择内存资源配置较高的从库），用于级联其他的从库。

选择一些从库（例如三分之一的从库），在这些从库上执行如下命令，让它们和刚才所选的从库，建立起主从关系。

```bash
replicaof 所选从库的IP 6379
```

这样一来，这些从库就会知道，在进行同步时，不用再和主库进行交互了，只要和级联的从库进行写操作同步就行了，这就可以减轻主库上的压力。

#### 总结

##### 全量同步和增量同步的区别？

全量同步：master将完整的内存数据生成RDB文件，发送RDB文件到slave。后续命令则记录在repl_baklog，逐个发送给slave；

增量同步：slave提交自己的offset到master，master获取到repl_baklog中从offset之后的命令给slave。

##### 全量同步和增量同步的执行时机？

**什么时候执行全量同步？**

1. slave节点第一次连接master时候；
2. slave节点断开时间太久了，repl_baklog中的offset已经被覆盖时候

**什么时候执行增量同步？**

slave节点断开又恢复，并且在repl_baklog中找到了对应的offset的时候

#####  repl_backlog_buffer 缓冲区是什么时候写入的呢？

在主服务器进行命令传播时，不仅会将写命令发送给从服务器，还会将写命令写入到 repl_backlog_buffer 缓冲区里，因此 这个缓冲区里会保存着最近传播的写命令。

> 发送给从服务器是命令传播，基于长连接的传播
>
> 写到repl_backlog_buffer 缓冲区，是为网络断开进行增量传播。

##### 为什么主从全量复制使用RDB而不使用AOF？

1、RDB文件内容是经过压缩的二进制数据，文件很小。而AOF文件记录的是每一次写操作的命令，写操作越多文件会变得很大，其中还包括很多对同一个key的多次冗余操作。在主从全量数据同步时，传输RDB文件可以尽量降低对主库机器网络带宽的消耗，从库在加载RDB文件时，一是文件小，读取整个文件的速度会很快，二是因为RDB文件存储的都是二进制数据，从库直接按照RDB协议解析还原数据即可，速度会非常快，而AOF需要依次重放每个写命令，这个过程会经历冗长的处理逻辑，恢复速度相比RDB会慢得多，所以使用RDB进行主从全量复制的成本最低。

2、假设要使用AOF做全量复制，意味着必须打开AOF功能，打开AOF就要选择文件刷盘的策略，选择不当会严重影响Redis性能。而RDB只有在需要定时备份和主从全量复制数据时才会触发生成一次快照。而在很多丢失数据不敏感的业务场景，其实是不需要开启AOF的。

##### 如何确定执行全量同步还是部分同步？

![image-20231212113415915](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231212113415915.png)

关键就是 `psync`的执行：

- 从节点根据当前状态，发送 `psync`命令给 master：
  - 如果从节点从未执行过 `replicaof` ，则从节点发送 `psync ? -1`，向主节点发送全量复制请求；
  - 如果从节点之前执行过 `replicaof` 则发送 `psync <runID> <offset>`, runID 是上次复制保存的主节点 runID，offset 是上次复制截至时从节点保存的复制偏移量。

- 主节点根据接受到的`psync`命令和当前服务器状态，决定执行全量复制还是部分复制：
  - runID 与从节点发送的 runID 相同，且从节点发送的 `slave_repl_offset`之后的数据在 `repl_backlog_buffer`缓冲区中都存在，则回复 `CONTINUE`，表示将进行部分复制，从节点等待主节点发送其缺少的数据即可；
  - runID 与从节点发送的 runID 不同，或者从节点发送的 slave_repl_offset 之后的数据已不在主节点的 `repl_backlog_buffer`缓冲区中 (在队列中被挤出了)，则回复从节点 `FULLRESYNC <runid> <offset>`，表示要进行全量复制，其中 runID 表示主节点当前的 runID，offset 表示主节点当前的 offset，从节点保存这两个值，以备使用。



#### 读写分离的问题

##### 数据不一致

###### 问题描述 

由于主从复制的命令传播是异步的，延迟与数据的不一致不可避免。

具体来说，在主从节点命令传播阶段，主节点收到新的写命令后，会发送给从节点。但是，主节点并不会等到从节点实际执行完命令后，再把结果返回给客户端，而是主节点自己在本地执行完命令后，就会向客户端返回结果了。如果从节点还没有执行主节点同步过来的命令，主从节点间的数据就不一致了。

###### 解决

- 在网络层面

尽量保证主从节点间的网络连接状况良好，避免主从节点在不同的机房。

- 外部监主从节点间的复制进度

Redis 的 INFO replication 命令可以查看主节点接收写命令的进度信息（master_repl_offset）和从节点复制写命令的进度信息（slave_repl_offset）。

因此，可以开发一个监控程序，先用 INFO replication 命令查到主、从节点的进度，然后，我们用 master_repl_offset 减去 slave_repl_offset，这样就能得到从节点和主节点间的复制进度差值了。

如果某个从节点的进度差值大于我们预设的阈值，我们可以让客户端不再和这个从节点连接进行数据读取，这样就可以减少读到不一致数据的情况。不过，为了避免出现客户端和所有从节点都不能连接的情况，我们需要把复制进度差值的阈值设置得大一些。

- 配置slave-serve-stale-data

如果为yes，当slave失去与master的连接，或正在拷贝中，slave会响应客户端的请求，此时数据可能不同步甚至没有数据；

如果为no，从服务器将阻塞所有请求，有客户端请求时返回“SYNC with master in progress”；

默认配置项：replica-serve-stale-data yes

##### 数据过期

###### 问题描述 

在单机版Redis中，存在两种删除策略：

- `惰性删除`：服务器不会主动删除数据，只有当客户端查询某个数据时，服务器判断该数据是否过期，如果过期则删除。
- `定期删除`：服务器执行定时任务删除过期数据，但是考虑到内存和CPU的折中（删除会释放内存，但是频繁的删除操作对CPU不友好），该删除的频率和执行时间都受到了限制。

但是在主从复制场景下，为了主从节点的数据一致性，从节点不会主动删除数据，而是由主节点控制从节点中过期数据的删除。由于主节点的惰性删除和定期删除策略，都不能保证主节点及时对过期数据执行删除操作，因此，当客户端通过Redis从节点读取数据时，很容易读取到已经过期的数据。

###### 解决

Redis 3.2 开始，通过从节点读取数据时，先判断数据是否已过期。如果过期则不返回客户端，并且删除数据。

将Redis升级到3.2可以解决数据过期问题。

##### 数据量过大的问题

###### 问题描述 

如果数据量过大，全量复制阶段主节点 fork + 保存 RDB 文件耗时过大，从节点长时间接收不到数据触发超时，主从节点的数据同步同样可能陷入**全量复制->超时导致复制中断->重连->全量复制->超时导致复制中断**……的循环。

###### 解决

主节点占用主机内存的比例也不应过大，最好只使用 50% - 65% 的内存，留下 30%-45% 的内存用于执行 bgsave 命令和创建复制缓冲区等。

可以采用主-从-从的方式减轻同步压力。

### 故障转移-哨兵机制

> 哨兵是 Redis 的一种运行模式，它专注于**对 Redis 实例（主节点、从节点）运行状态的监控，并能够在主节点发生故障时通过一系列的机制实现选主及主从切换，实现故障转移，确保整个 Redis 系统的可用性**。

#### 哨兵架构

##### 单哨兵架构

![image-20231211194147069](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231211194147069.png)

##### 多哨兵架构

![image-20231211194232226](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231211194232226.png)

#### 哨兵机制原理

Redis 哨兵具备的能力有如下几个：

- **监控**：持续监控 master 、slave 是否处于预期工作状态。
- **自动切换主库**：当 Master 运行故障，哨兵启动自动故障恢复流程：从 slave 中选择一台作为新 master。
- **通知**：让 slave 执行 replicaof ，与新的 master 同步；并且通知客户端与新 master 建立连接。

##### 配置

哨兵其实主动复制的自动版，所以需要先配置好主从复制，不同点在于要增加几个哨兵进行监控。

创建哨兵文件，指定监听的主节点ip和端口：

```conf
# sentinel1.conf
port 26379
sentinel monitor mymaster 127.0.0.1 6379 1
```

```conf
# sentinel2.conf
port 26380
sentinel monitor mymaster 127.0.0.1 6379 1
```

启动哨兵：

```shell
$ redis-sentinel sentinel1.conf
$ redis-sentinel sentinel2.conf
```

##### 监控

![ ](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/26f88373d8454682b9e0c1d4fd1611b4-20230309233114856.png)

哨兵会每隔 1 秒给所有主从节点发送 PING 命令，当主从节点收到 PING 命令后，会发送一个响应命令给哨兵，这样就可以判断它们是否在正常运行。

###### 主观下线


如果主节点或者从节点没有在规定的时间内响应哨兵的 PING 命令，哨兵就会将它们标记为「**主观下线**」。这个「规定的时间」是配置项 `down-after-milliseconds` 参数设定的，单位是毫秒。

###### 客观下线

如果该节点是「主节点」，是因为有可能「主节点」其实并没有故障，可能只是因为主节点的系统压力比较大或者网络发送了拥塞，导致主节点没有在规定时间内响应哨兵的 PING 命令。

  为了减少误判的情况，哨兵在部署的时候不会只部署一个节点，而是用多个节点部署成**哨兵集群**，**通过多个哨兵节点一起判断，就可以就可以避免单个哨兵因为自身网络状况不好，而误判主节点下线的情况**。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。

  当一个哨兵判断主节点为「主观下线」后，就会向其他哨兵发起命令，其他哨兵收到这个命令后，就会根据自身和主节点的网络状况，做出赞成投票或者拒绝投票的响应。

  当这个哨兵的赞同票数达到哨兵配置文件中的 quorum 配置项设定的值后，这时主节点就会被该哨兵标记为「客观下线」

>[!ATTENTION]客观下线只适用于主节点。

例如，现在有 3 个哨兵，quorum 配置的是 2，那么一个哨兵需要 2 张赞成票，就可以标记主节点为“客观下线”了。这 2 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。

<font color=red>quorum 的值一般设置为哨兵个数的二分之一加 1</font>，例如 3 个哨兵就设置 2。

##### 故障转移

> 在哨兵集群中通过投票的方式，选举出了哨兵 leader 后，由哨兵 leader 进行主从故障转移

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/%E4%B8%BB%E4%BB%8E%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB.png)

主从故障转移操作包含以下四个步骤：

- 第一步：在已下线主节点（旧主节点）属下的所有「从节点」里面，挑选出一个从节点，并将其转换为主节点。
- 第二步：让已下线主节点属下的所有「从节点」修改复制目标，修改为复制「新主节点」；
- 第三步：将新主节点的 IP 地址和信息，通过「发布者/订阅者机制」通知给客户端；
- 第四步：继续监视旧主节点，当这个旧主节点重新上线时，将它设置为新主节点的从节点

###### 选主

![image-20231212142301281](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231212142301281.png)

故障转移操作第一步要做的就是在已下线主节点属下的所有「从节点」中，挑选出一个状态良好、数据完整的从节点，然后向这个「从节点」发送 SLAVEOF no one 命令，将这个「从节点」转换为「主节点」。

- 过滤网络连接不好的节点

Redis 有个叫 down-after-milliseconds 配置项，其 down-after-milliseconds 是主从节点断连的最大连接超时时间。

如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，就可以认为主从节点断连了。

如果发生断连的次数超过了 10 次，就说明这个从节点的网络状况不好，不适合作为新主节点，将该节点过滤掉。

- 再根据优先级排序

slave-priority 配置项，可以给从节点设置优先级。

每一台从节点的服务器配置不一定是相同的，我们可以根据服务器性能配置来设置从节点的优先级。

比如，如果「A 从节点」的物理内存是所有从节点中最大的，那么我们可以把「A 从节点」的优先级设置成最高。这样当哨兵进行第一轮考虑的时候，优先级最高的 A 从节点就会优先胜出，于是就会成为新主节点。

- 优先级相同的，再根据复制进度排序

在主从同步架构中，会用从节点的offset标记同步进度，如果某个从节点的 slave_repl_offset 最接近主节点的 master_repl_offset，说明它的复制进度是最靠前的，于是就可以将它选为新主节点。

- 复制进度也相同的，则选择ID号较小的

每个从节点都有一个编号，这个编号就是 ID 号，是用来唯一标识从节点的。

###### 更换新主节点的身份

在选举出新的主节点后，哨兵 leader 向被选中的主节点发送 `SLAVEOF no one` 命令，让这个从节点解除从节点的身份，将其变为新主节点。

在发送 `SLAVEOF no one` 命令之后，哨兵 leader 会以每秒一次的频率向被升级的从节点发送 `INFO` 命令（没进行故障转移之前，`INFO` 命令的频率是每十秒一次），并观察命令回复中的角色信息，当被升级节点的角色信息从原来的 slave 变为 master 时，哨兵 leader 就知道被选中的从节点已经顺利升级为主节点了。

###### 其他从节点指向新的主节点

当新主节点出现之后，哨兵 leader 下一步要做的就是，让其他「从节点」指向「新主节点」，这一动作可以通过向「从节点」发送 `SLAVEOF` 命令来实现。

###### 通知客户主节点已经更换

过前面一系列的操作后，哨兵集群完成了主从切换的工作，下一步需要**通过 Redis 的发布者/订阅者机制**通知客户端，主节点已经更换。

每个哨兵节点提供发布者/订阅者机制，客户端可以从哨兵订阅消息。

客户端和哨兵建立连接后，客户端会订阅哨兵提供的频道。**主从切换完成后，哨兵就会向 `+switch-master` 频道发布新主节点的 IP 地址和端口的消息，这个时候客户端就可以收到这条信息，然后用这里面的新主节点的 IP 地址和端口进行通信了**。

哨兵提供的消息订阅频道有很多，不同频道包含了主从节点切换过程中的不同关键事件，几个常见的事件如下：

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/%E5%93%A8%E5%85%B5%E9%A2%91%E9%81%93.webp)

###### 监视旧主节点

故障转移操作最后要做的是，继续监视旧主节点，当旧主节点重新上线时，哨兵集群就会向它发送 `SLAVEOF` 命令，让它成为新主节点的从节点。

#### 哨兵集群

##### 集群组件

###### 配置

在配置哨兵集群的时候，哨兵配置中只设置了监控的 master  Redis IP 和 port，并没有配置其他哨兵的连接信息。

```
sentinel monitor <master-name> <ip> <redis-port> <quorum>
```

###### 哨兵之间通信

首先，看这个问题：不需要填其他哨兵节点的信息，我就好奇它们是如何感知对方的，又是如何组成哨兵集群的？

哨兵实例之间可以相互发现，要归功于 Redis 提供的 pub/sub 机制，也就是发布 / 订阅机制。

在主从集群中，主库上有一个名为`__sentinel__:hello`的频道，不同哨兵就是通过它来相互发现，实现互相通信的。

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/a6286053c6884cf58bf397d01674fe80.png)

哨兵 1 把自己的 IP（172.16.19.3）和端口（26579）发布到`__sentinel__:hello`频道上，哨兵 2 和 3 订阅了该频道。那么此时，哨兵 2 和 3 就可以从这个频道直接获取哨兵 1 的 IP 地址和端口号。同样，哨兵 2、3 可以和哨兵 1 建立网络连接。

###### 集群如何拿到从节点信息

哨兵集群会对「从节点」的运行状态进行监控，那哨兵集群如何知道「从节点」的信息？

主节点知道所有「从节点」的信息，所以哨兵会每 10 秒一次的频率向主节点发送 INFO 命令来获取所有「从节点」的信息，在主节点里获得了所有从节点连接信息，于是就能和从节点建立连接，并进行监控了。

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/fdd5f695bb3643258662886f9fba0aab.png)

哨兵 B 给主节点发送 INFO 命令，主节点接受到这个命令后，就会把从节点列表返回给哨兵。接着，哨兵就可以根据从节点列表中的连接信息，和每个从节点建立连接，并在这个连接上持续地对从节点进行监控。哨兵 A 和 C 可以通过相同的方法和从节点建立连接。同时，哨兵又通过 INFO 命令，在主节点里获得了所有从节点连接信息，于是就能和从节点建立连接，并进行监控了。

##### 哨兵leader选举

###### 候选者

哪个哨兵节点判断主节点为「客观下线」，这个哨兵节点就是候选者。

假设有三个哨兵。当哨兵 B 先判断到主节点「主观下线后」，就会给其他实例发送 is-master-down-by-addr 命令。接着，其他哨兵会根据自己和主节点的网络连接情况，做出赞成投票或者拒绝投票的响应。

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/d0bed80d28a543fd8dcd299d4b06cf04.png)

当哨兵 B 收到赞成票数达到哨兵配置文件中的 quorum 配置项设定的值后，就会将主节点标记为「客观下线」，此时的哨兵 B 就是一个 Leader 候选者。

###### 选举

候选者会向其他哨兵发送命令，表明希望成为 Leader 来执行主从切换，并让所有其他哨兵对它进行投票。

哨兵的选举机制采用Raft选举算法， **选举的票数大于等于num(sentinels)/2+1时，将成为leader，如果没有超过，继续选举，**其流程为：

1. 每个在线的哨兵节点都有资格成为领导者，当它确认主节点主观下线时候，会向其他哨兵节点发送自己成为leader的请求。
2. 收到请求的哨兵节点，如果之前还没有同意过其他哨兵节点leader请求的行为，将同意该请求，否则拒绝。
3. 当某个该哨兵节点发现自己已经获得多数的票数，则成为领导者。
4. 如果此过程没有选举出领导者，等待选举时间超时后发起下一次选举。

每个哨兵只有一次投票机会，如果用完后就不能参与投票了，可以投给自己或投给别人，但是<font color=red>**只有候选者才能把票投给自己**</font>。

因此，投票过程中，任何一个「候选者」想成为leader，要满足两个条件：

- 拿到半数以上的赞成票；
- 拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。

###### 思考

如果某个时间点，刚好有两个哨兵节点判断到主节点为客观下线，此时有两个候选者了，这时该如何决定谁是 Leader 呢？

每位候选者都会先给自己投一票，然后向其他哨兵发起投票请求。如果投票者先收到「候选者 A」的投票请求，就会先投票给它，如果投票者用完投票机会后，收到「候选者 B」的投票请求后，就会拒绝投票。这时，候选者 A 先满足了上面的那两个条件，所以「候选者 A」就会被选举为 Leader。

#### 总结

##### 哨兵节点至少要有 3 个？

如果哨兵集群中只有 2 个哨兵节点，此时如果一个哨兵想要成功成为 Leader，必须获得 2 票，而不是 1 票。

如果哨兵集群中有个哨兵挂掉了，那么就只剩一个哨兵了，如果这个哨兵想要成为 Leader，这时票数就没办法达到 2 票，就无法成功成为 Leader，这时是无法进行主从节点切换的。

因此，至少会配置 3 个哨兵节点。这时，如果哨兵集群中有个哨兵挂掉了，那么还剩下两个哨兵，如果这个哨兵想要成为 Leader，这时还是有机会达到 2 票的，不会导致无法进行主从节点切换。

##### 客观下线和能否主从切换概念

Redis 1主4从，5个哨兵，哨兵配置quorum为2，如果3个哨兵故障，当主库宕机时，哨兵能否判断主库“客观下线”？能否自动切换？

- 客观下线的条件是大于等于quorum，这个参数是可以配置的，一般建议哨兵个数的二分之一加 1

哨兵集群可以判定主库“主观下线”。5个哨兵挂掉3个，还剩下两个哨兵，由于quorum=2，所以当一个哨兵判断主库“主观下线”后，询问另外一个哨兵后也会得到同样的结果，2个哨兵都判定“主观下线”，达到了quorum的值，因此，**哨兵集群可以判定主库为“客观下线”**。

- 能否主从切换，<font color=red>是在满足可以客观下线的前提下，额外满足哨兵leader是否可以选取出来，</font>是确定的要求大于等于哨兵个数的二分之一加 1，这个数是不可配置的

**哨兵不能完成主从切换**。哨兵标记主库“客观下线后”，在选举“哨兵领导者”时，一个哨兵必须拿到超过多数的选票(5/2+1=3票)。但目前只有2个哨兵活着，无论怎么投票，一个哨兵最多只能拿到2票，永远无法达到`N/2+1`选票的结果。

> [!Note]能主从切换，一定可以满足客观下线
>
> 满足客观下线，不一定能主从切换

## 高可用+可拓展-Redis集群（todo）

> 主从复制和哨兵机制保障了高可用，就读写分离而言虽然slave节点扩展了主从的读并发能力，但是**写能力**和**存储能力**是无法进行扩展，就只能是master节点能够承载的上限。
>
> 如果面对海量数据那么必然需要构建master（主节点分片)之间的集群，同时必然需要吸收高可用（主从复制和哨兵机制）能力，即每个master分片节点还需要有slave节点，这是分布式系统中典型的纵向扩展（集群的分片技术）的体现。
>
> 在 Redis 3.0版本中对应的设计就是Redis Cluster
>
> 因此，Redis Cluster方案同时满足了高可用和可拓展的特性。

### 使用

#### 配置Demo

##### 架构

假定，3台服务器，每台服务器开启2台实例构建基础主从，共6个Redis实例，三主三从。

> 根据官方：要让集群正常运作至少需要三个主节点，不过在刚开始试用集群功能时， 强烈建议使用六个节点： 其中三个为主节点， 而其余三个则是各个主节点的从节点。

##### 配置文件

> [!ATTENTION]
>
> 注意！Redis普通服务会有2套配置文件，一套为普通服务配置文件，一套为集群服务配置文件。
>
> Cluster模式下书写的集群配置文件

在`/data/redis/redisConf`目录下创建创建节点所需要的配置文件

```yaml
# 修改为后台启动
daemonize yes
# 修改端口号
port 8001
# 指定数据文件存储位置
dir /usr/local/redis/8001/
# 开启集群模式
cluster-enabled yes
# 集群节点信息文件配置
cluster-config-file nodes-8001.conf
# 集群节点超时间
cluster-node-timeout 15000
# 去掉bind绑定地址
# bind 127.0.0.1 -::1 (这里没写错就是加#注释掉bind配置)
# 关闭保护模式
protected-mode no
# 开启aof模式持久化
appendonly yes
# 设置连接Redis需要密码123（选配）
requirepass 123456
# 设置Redis节点与其他节点之间访问需要密码123（选配）
masterauth 123456
```

- cluster-enabled yes

如果配置yes则开启集群功能，此redis实例作为集群的一个节点，否则，它是一个普通的单一的redis实例。
- cluster-config-file nodes-8001.conf

虽然此配置的名字叫"集群配置文件"，但是此配置文件不能人工编辑，它是集群节点自动维护的文件，主要用于记录集群中有哪些节点、他们的状态以及一些持久化参数等，方便在重启时恢复这些状态。通常是在收到请求之后这个文件就会被更新。
- cluster-node-timeout 15000

这是集群中的节点能够失联的最大时间，超过这个时间，该节点就会被认为故障。如果主节点超过这个时间还是不可达，则用它的从节点将启动故障迁移，升级成主节点。注意，任何一个节点在这个时间之内如果还是没有连上大部分的主节点，则此节点将停止接收任何请求。一般设置为15秒即可。
- cluster-slave-validity-factor 10

如果设置成０，则无论从节点与主节点失联多久，从节点都会尝试升级成主节点。如果设置成正数，则cluster-node-timeout乘以cluster-slave-validity-factor得到的时间，是从节点与主节点失联后，此从节点数据有效的最长时间，超过这个时间，从节点不会启动故障迁移。

假设cluster-node-timeout=5，cluster-slave-validity-factor=10，则如果从节点跟主节点失联超过50秒，此从节点不能成为主节点。注意，如果此参数配置为非0，将可能出现由于某主节点失联却没有从节点能顶上的情况，从而导致集群不能正常工作，在这种情况下，只有等到原来的主节点重新回归到集群，集群才恢复运作。

- cluster-migration-barrier 1

主节点需要的最小从节点数，只有达到这个数，主节点失败时，它从节点才会进行迁移。更详细介绍可以看本教程后面关于副本迁移到部分。

- cluster-require-full-coverage yes

在部分key所在的节点不可用时，如果此参数设置为"yes"(默认值), 则整个集群停止接受操作；如果此参数设置为”no”，则集群依然为可达节点上的key提供读操作。

##### 启动redis

执行以下命令进行服务启动：

```shell
$ redis-server /usr/local/redis/conf/redis.cnf
```

在启动集群时，会按照Redis服务配置文件的配置项判断是否启动集群模式，如图所示：

![image-20210401221228929](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20210401221228929.png)

现在虽然说每个服务都成功启动了，但是彼此之间并没有任何联系。所以下一步要做的就是将6个服务加入至一个集群。

##### 加入集群

在任意一台机器上执行如下命令，即可创建集群。

Redis会随机分配主从机器，并且在分配的时Redis是不会让主节点与从节点在同一台机器上的。

```shell
# -a 密码认证，若没写密码无效带这个参数
# --cluster create 创建集群实例列表 IP:PORT IP:PORT IP:PORT
# --cluster-replicas 复制因子1（即每个主节点需1个从节点）
./bin/redis-cli -a 123456 --cluster create --cluster-replicas 1 192.168.100.101:8001 192.168.100.101:8002 192.168.100.102:8003 192.168.100.102:8004 192.168.100.103:8005 192.168.100.103:8006
```

![image-20231213145832633](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231213145832633.png)

##### 链接集群

```shell
# -a 密码认证
# -c 连接集群
# -h 集群中任意一个Redis节点IP
# -p 集群中任意一个Redis节点端口
./bin/redis-cli -a 123456 -c -h 192.168.100.101 -p 8001
```

#### cluster 命令

##### 集群
- cluster info  


打印集群的信息

> 127.0.0.1:8001> cluster info
>
> cluster_state:ok       # 如果当前redis发现有failed的slots，默认为把自己cluster_state从ok个性为fail, 写入命令会失败。如果设置cluster-require-full-coverage为no,则无此限制。
> cluster_slots_assigned:16384  #已分配的槽
> cluster_slots_ok:16384        #槽的状态是ok的数目
> cluster_slots_pfail:0           #可能失效的槽的数目
> cluster_slots_fail:0            #已经失效的槽的数目
> cluster_known_nodes:6       #集群中节点个数
> cluster_size:3                #集群中设置的分片个数
> cluster_current_epoch:15      #集群中的currentEpoch总是一致的,currentEpoch越高，代表节点的配置或者操作越新,集群中最大的那个node epoch
> cluster_my_epoch:12         #当前节点的config epoch，每个主节点都不同，一直递增, 其表示某节点最后一次变成主节点或获取新slot所有权的逻辑时间.
> cluster_stats_messages_sent:270782059
> cluster_stats_messages_received:270732696

- cluster nodes 


列出集群当前已知的所有节点，以及这些节点的相关信息  

> 127.0.0.1:8001> cluster nodes
> 25e8c9379c3db621da6ff8152684dc95dbe2e163 192.168.64.102:8002 master - 0 1490696025496 15 connected 5461-10922
>
> d777a98ff16901dffca53e509b78b65dd1394ce2 192.168.64.156:8001 slave 0b1f3dd6e53ba76b8664294af2b7f492dbf914ec 0 1490696027498 12 connected
>
> 8e082ea9fe9d4c4fcca4fbe75ba3b77512b695ef 192.168.64.108:8000 master - 0 1490696025997 14 connected 0-5460
>
> 0b1f3dd6e53ba76b8664294af2b7f492dbf914ec 192.168.64.170:8001 myself,master - 0 0 12 connected 10923-16383
>
> eb8adb8c0c5715525997bdb3c2d5345e688d943f 192.168.64.101:8002 slave 25e8c9379c3db621da6ff8152684dc95dbe2e163 0 1490696027498 15 connected
>
> 4000155a787ddab1e7f12584dabeab48a617fc46 192.168.67.54:8000 slave 8e082ea9fe9d4c4fcca4fbe75ba3b77512b695ef 0 1490696026497 14 connected
>
> 说明
>
> - 节点ID：例如25e8c9379c3db621da6ff8152684dc95dbe2e163
> - ip:port：节点的ip地址和端口号，例如192.168.64.102:8002
> - flags：节点的角色(master,slave,myself)以及状态(pfail,fail)，如果节点是一个从节点的话，那么跟在flags之后的将是主节点的节点ID，例如192.168.64.156:8001主节点的ID就是0b1f3dd6e53ba76b8664294af2b7f492dbf914ec
> - 集群最近一次向节点发送ping命令之后，过了多长时间还没接到回复
> - 节点最近一次返回pong回复的时间
> - 节点的配置纪元(config epoch)
> - 本节点的网络连接情况
> - 节点目前包含的槽，例如192.168.64.102:8002目前包含的槽为5461-10922
>
> ![image-20231213151545683](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231213151545683.png)

##### 节点(node)  

- cluster meet <ip> <port>    

将ip和port所指定的节点添加到集群当中，让它成为集群的一份子  

- cluster forget <node_id>     

从集群中移除node_id指定的节点

- cluster replicate <node_id>  

将当前节点设置为node_id指定的节点的从节点

- cluster saveconfig          

将节点的配置文件保存到硬盘里面

- cluster slaves <node_id>   

 列出该slave节点的master节点

- cluster set-config-epoch     

强制设置configEpoch 

##### 槽(slot)  

- cluster addslots <slot> [slot ...]             

将一个或多个槽(slot)指派(assign)给当前节点

- cluster delslots <slot> [slot ...]              

移除一个或多个槽对当前节点的指派 

- cluster flushslots                        

 移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点 

- cluster setslot <slot> node <node_id>      

 将槽slot指派给node_id指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽，然后再进行指派 

- cluster setslot <slot> migrating <node_id>  

将本节点的槽slot迁移到node_id指定的节点中  

- cluster setslot <slot> importing <node_id>  

从node_id 指定的节点中导入槽slot到本节点 

- cluster setslot <slot> stable               

取消对槽slot的导入(import)或者迁移(migrate) 

##### 键(key)  

- cluster keyslot <key>                    

计算键key应该被放置在哪个槽上  

- cluster countkeysinslot <slot>             

返回槽slot目前包含的键值对数量 

- cluster getkeysinslot <slot> <count>        

返回count个slot槽中的键

##### 其它

- cluster myid    

返回节点的ID

- cluster slots    

返回节点负责的slot

- cluster reset    

重置集群，慎用

#### 集群运维

##### 新增主节点

###### 加入集群

```shell
# 使用如下命令即可添加节点将一个新的节点添加到集群中
# -a 密码认证(没有密码不用带此参数)
# --cluster add-node 添加节点 新节点IP:新节点端口 任意存活节点IP:任意存活节点端口
./bin/redis-cli -a 123456 --cluster add-node 192.168.100.104:8007 192.168.100.101:8001
```

![image-20231213151915289](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231213151915289.png)

使用`cluster nodes`命令查看集群信息表，可以看到8007已经被添加到了新的集群中了，但是8007并且没有任何的槽位信息，这时就需要迁移槽位。

![image-20231213152022511](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231213152022511.png)

###### 迁移槽位

```shell
# 使用如下命令将其它主节点的分片迁移到当前节点中
# -a 密码认证(没有密码不用带此参数)
# --cluster reshard 槽位迁移 从节点IP:节点端口，中迁移槽位到当前节点中
./bin/redis-cli --cluster reshard 192.168.100.101:8002
```

执行该命令后，会以输入的方式要求输入：

- 给哪个节点迁移
- 迁移多少槽位

![image-20231213152203692](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231213152203692.png)

##### 增加从节点

###### 加入集群

```shell
# 使用如下命令即可添加节点将一个新的节点添加到集群中
# -a 密码认证(没有密码不用带此参数)
# --cluster add-node 添加节点 新节点IP:新节点端口 任意存活节点IP:任意存活节点端口
./bin/redis-cli -a 123456 --cluster add-node 192.168.100.104:8008 192.168.100.101:8001
```

###### 从节点配置

在新增的从节点上，设置为跟随哪个主节点。

客户端命令连接到刚刚新添加的8008节点的上，并且为他设置一个主节点，设置完毕后再次查看节点信息，可以看到8008已经是8007的从节点了。

```shell
# 连接需设为从节点的Redis服务
./bin/redis-cli -a 123456 -p 8008
# 将当前节点分配为 8cf44439390dc9412813ad27c43858a6bb53365c 的从节点
CLUSTER REPLICATE 8cf44439390dc9412813ad27c43858a6bb53365c
```

![image-20231213152619657](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231213152619657.png)

##### 删除主节点

主节点删除就那么首先需要对槽进行迁移，如当前需要移除8007节点，那么首先需要把8007的节点槽位移动到别的节点中，才能删除。

###### 迁移槽位

把要删除的槽位迁移到其他节点上。

![image-20231213152739888](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20231213152739888.png)

###### 删除主节点

```shell
# 执行如下命令删除节点
# -a 密码认证(没有密码不用带此参数)
# --cluster del-node 连接任意一个存活的节点IP:连接任意一个存活的节点端口 要删除节点ID 
./bin/redis-cli -a 123456 --cluster del-node 192.168.100.101:8002 8cf44439390dc9412813ad27c43858a6bb53365c
```

##### 删除从节点

从节点删除比较简单，直接删除即可，现在要删除8008节点。

```shell
# -a 密码认证(没有密码不用带此参数)
# --cluster del-node 连接任意一个存活的节点IP:连接任意一个存活的节点端口 要删除节点ID 
./bin/redis-cli -a 123456 --cluster del-node 192.168.100.104:8008 71cb4fe842e83252f0ffabdc2b31eddb98fd4c89
```

##### 重新分配槽位

`重新分配槽位慎用！！！`，该功能可以让着集群的槽位重新平均分配但是由于涉及到槽位大量迁移会导致整个Redis阻塞停止处理客户端的请求。

```shell
# -a 密码认证(没有密码不用带此参数)
# --cluster rebalance 重新分配集群中的槽位
./bin/redis-cli -a 123456 --cluster rebalance 192.168.100.101:8002
```

#### SpringBoot集成

##### pom

```xml
        <!--springboot中的redis依赖-->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-redis</artifactId>
        </dependency>
        <!-- lettuce pool 缓存连接池-->
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-pool2</artifactId>
        </dependency>
```

##### 配置文件

```yml
#端口，项目上下文
server:
  port: 8080
  servlet:
    context-path: /redis-demo

spring:
  redis:
#    host: 192.168.223.131
#    port: 7001
    password: admin@2021
    # Redis 默认数据库设置
    database: 0
    # Redis Cluster集群节点配置
    cluster:
      # Redis 集群地址信息
      nodes:
        - 192.168.223.131:7001
        - 192.168.223.131:7002
        - 192.168.223.131:7003
        - 192.168.223.131:7004
        - 192.168.223.131:7005
        - 192.168.223.131:7006
      # 获取失败 最大重定向次数
      max-redirects: 3
    #如果用以前的jedis，可以把下面的lettuce换成jedis即可
    lettuce:
      pool:
        # 连接池最大连接数默认值为8
        max-active: 1000
        # 连接池最大阻塞时间（使用负值表示没有限制）默认值为-1
        max-wait: -1
        # 连接池中最大空闲连接数默认值为8
        max-idle: 10
        # 连接池中的最小空闲连接数，默认值为0
        min-idle: 10
```

##### RedisConfig类

```java
package com.demo.config;

import org.springframework.cache.annotation.CachingConfigurerSupport;
import org.springframework.cache.interceptor.KeyGenerator;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.data.redis.serializer.GenericJackson2JsonRedisSerializer;
import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer;

import java.util.Arrays;


@Configuration
public class RedisConfig extends CachingConfigurerSupport {

    @Bean
    public RedisTemplate<String, Object>  redisTemplate(RedisConnectionFactory redisConnectionFactory) {
        RedisTemplate<String, Object>  template = new RedisTemplate<>();
        template.setConnectionFactory(redisConnectionFactory);
        //对象的序列化，GenericJackson2JsonRedisSerializer实现了RedisSerializer接口
        GenericJackson2JsonRedisSerializer serializer = new GenericJackson2JsonRedisSerializer();
        template.setDefaultSerializer(serializer);
        return template;
    }

    /**
     * 重写key的生成策略，用【类名+方法名+参数名】这样就可以保证key不为空
     * @return
     */
    @Bean
    @Override
    public KeyGenerator keyGenerator() {
        return (target, method, objects) -> {
            StringBuilder sb = new StringBuilder();
            sb.append(target.getClass().getName()).append(".").append(method.getName()).append(Arrays.toString(objects));
            return sb.toString();
        };
    }
}
```

### 原理

#### 分片/分区机制（可拓展）

##### 分布式分区方法

>  [!note]在分布式理论中，分区方法包含多种，范围分片、哈希分区、一致性哈希等等，详细可见分布式理论。
>
> 在Redis Cluster中采用的分区技术叫做虚拟槽分区，可以算上面虚拟一致性哈希分区的变种，在本章节重点关注虚拟槽分区。
>
> 以前思想有个误区，以为Redis中具有范围分区、哈希分区等等分区手段，其实这个都是分布式系统中的理论技术。如果自行搭建多个Redis + 实现客户端/代理中间件分区算法的话，可以选择这些技术，但这样工作量极大。如果是使用Redis Cluster成熟方案，分区技术即只有虚拟槽分区这一种。

- 节点取余分区

使用特定的数据，如Redis的键或用户ID，再根据节点数量N使用公式:
hash(key)%N计算出哈希值，用来决定数据映射到哪一个节点上。这种方案存在一个问题:当节点数量变化时，如扩容或收缩节点，数据节点映射关系需要重新计算，会导致数据的重新迁移。

这种方式的突出优点是简单性，常用于数据库的分库分表规则，一般采用预分区的方式，提前根据数据量规划好分区数，比如划分为512或1024张表，保证可支撑未来一段时间的数据量,再根据负载情况将表迁移到其他数据库中。扩容时通常采用翻倍扩容，避免数据映射全部被打乱导致全量迁移的情况。

- 一致性哈希分区

一致性哈希分区（ Distributed Hash Table)实现思路是为系统中每个节点分配一个 token,范围一般在0~23，这些token构成一个哈希环。数据读写执行节点查找操作时，先根据key计算hash值，然后顺时针找到第一个大于等于该哈希值的token节点。例如：

集群中有三个节点（Node1、Node2、Node3），五个键（key1、key2、key3、key4、key5），其路由规则为：

![image.png](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/bab557dc5d9c4932bc2682481a50e8b7.png)

当集群中增加节点时，比如当在Node2和Node3之间增加了一个节点Node4，此时再访问节点key4时，不能在Node4中命中，更一般的，介于Node2和Node4之间的key均失效，这样的失效方式太过于“集中”和“暴力”，更好的方式应该是“平滑”和“分散”地失效。

![image.png](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/9de42d7a40a94ca5ba61daff1d0cd894.png)

这种方式相比节点取余最大的好处在于加入和删除节点只影响哈希环中相邻的节点，对其他节点无影响。但一致性哈希分区存在几个问题:

1、当使用少量节点时，节点变化将大范围影响哈希环中数据映射，因此这种方式不适合少量数据节点的分布式方案。

2、增加节点只能对下一个相邻节点有比较好的负载分担效果，例如上图中增加了节点Node4只能够对Node3分担部分负载，对集群中其他的节点基本没有起到负载分担的效果；类似地，删除节点会导致下一个相邻节点负载增加，而其他节点却不能有效分担负载压力。

正因为一致性哈希分区的这些缺点，一些分布式系统采用虚拟槽对一致性哈希进行改进，比如虚拟一致性哈希分区。

- 虚拟一致性哈希分区

为了在增删节点的时候，各节点能够保持动态的均衡，将每个真实节点虚拟出若干个虚拟节点，再将这些虚拟节点随机映射到环上。此时每个真实节点不再映射到环上，真实节点只是用来存储键值对，它负责接应各自的一组环上虚拟节点。当对键值对进行存取路由时，首先路由到虚拟节点上，再由虚拟节点找到真实的节点。

如下图所示，三个节点真实节点：Node1、Node2和Node3，每个真实节点虚拟出三个虚拟节点：X#V1、X#V2和X#V3，这样每个真实节点所负责的hash空间不再是连续的一段，而是分散在环上的各处，这样就可以将局部的压力均衡到不同的节点，虚拟节点越多，分散性越好，理论上负载就越倾向均匀。

![image.png](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/70c7b32ede304da4bf8498c39e836bc1.png)

##### 虚拟槽分区

###### 槽

槽Slot是集群内数据管理和迁移的基本单位，主要目的是为了方便数据拆分和集群扩展，每个节点会负责一定数量的槽。

###### 槽和key值

根据key值计算，该key应该存放在哪个槽：

`slot=CRC16(key) &16383`

![image.png](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/a3a1240b7e74478abba33cd4a6e5821d.png)

###### 槽的个数

通过上面的哈希函数，也可以看出来RedisCluster槽范围是0 ～16383。

为什么槽的范围是0 ～16383，也就是说槽的个数在16384个？redis的作者在github上有个回答：[https://github.com/redis/redis/issues/2576](https://github.com/redis/redis/issues/2576)

![image.png](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/1d9939c506b34a9aa5cd004fb83c34c7.png)

在ping/pong中，需要携带一定数量的其他节点信息用于交换，节点数量越多，消息体内容越大，内部通信占有的带宽也就越大，因此节点数量不能过多，不建议redis cluster节点数量超过1000个。

在redis节点发送心跳包时需要把所有的槽放到这个心跳包里，以便让节点知道当前集群信息，16384=16k，在发送心跳包时使用char进行bitmap压缩后是:16384 ÷8÷1024=2k，也就是说可以使用2k的空间创建了16k的槽数。如果槽数过大，ping消息的消息头太大了，浪费带宽。

虽然使用CRC16算法最多可以分配65535（2^16-1）个槽位，65535=65k，压缩后就是8k，也就是说需要需要8k的心跳包，作者认为这样做不太值得。同时，对于节点数在1000以内的redis cluster集群，16384个槽位够用了，可以以确保每个 master 有足够的插槽，没有必要。

###### 槽和节点

每个节点会负责一定数量的槽。

比如集群有3个节点，则每个节点平均大约负责5460个槽。

![image.png](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/d8360eb94ffc458294f0f003a7f94380.png)

##### Redis 虚拟槽分区的特点

1、解耦数据和节点之间的关系,简化了节点扩容和收缩难度。

2、节点自身维护槽的映射关系，不需要客户端或者代理服务维护槽分区元数据。口支持节点、槽、键之间的映射查询,用于数据路由、在线伸缩等场景。

#### 节点通信

##### Gossip协议

Redis 集群是去中心化的，彼此之间状态同步靠 gossip 协议通信。

Gossip协议的最大的好处是，**即使集群节点的数量增加，每个节点的负载也不会增加很多，几乎是恒定的。**

GOSSIP集群的消息有以下几种类型：

- `Meet` 通过「cluster meet ip port」命令，已有集群的节点会向新的节点发送邀请，加入现有集群。
- `Ping` 节点每秒会向集群中其他节点发送 ping 消息，消息中带有自己已知的两个节点的地址、槽、状态信息、最后一次通信时间等。
- `Pong` 节点收到 ping 消息后会回复 pong 消息，消息中同样带有自己已知的两个节点信息。
- `Fail` 节点 ping 不通某节点后，会向集群所有节点广播该节点挂掉的消息。其他节点收到消息后标记已下线。

##### 消息格式

所有的消息格式划分为：消息头和消息体。

###### 消息头

集群内所有的消息都采用相同的消息头结构`clusterMsg`，它包含了发送节点关键信息，如节点id、槽映射、节点标识(主从角色，是否下线）等。消息头包含发送节点自身状态数据，接收节点根据消息头就可以获取到发送节点的相关数据。

###### 消息体

消息体在Redis内部采用`clusterMsg Data` 结构声明，定义发送消息的数据。

一共有四类，其中ping、meet、pong都采用clusterMsgDataGossip结构来表示。

```c
typedef struct {
    // 节点的名字
    // 在刚开始的时候，节点的名字会是随机的
    // 当 MEET 信息发送并得到回复之后，集群就会为节点设置正式的名字
    char nodename[REDIS_CLUSTER_NAMELEN];
    // 最后一次向该节点发送 PING 消息的时间戳
    uint32_t ping_sent;
    // 最后一次从该节点接收到 PONG 消息的时间戳
    uint32_t pong_received;
    // 节点的 IP 地址
    char ip[REDIS_IP_STR_LEN];    /* IP address last time it was seen */
    // 节点的端口号
    uint16_t port;  /* port last time it was seen */
    // 节点的标识值
    uint16_t flags;
    // 对齐字节，不使用
    uint32_t notused; /* for 64 bit alignment */
} clusterMsgDataGossip;
 
typedef struct {
    // 下线节点的名字
    char nodename[REDIS_CLUSTER_NAMELEN];
} clusterMsgDataFail;
 
typedef struct {
    // 频道名长度
    uint32_t channel_len;
    // 消息长度
    uint32_t message_len;
    // 消息内容，格式为 频道名+消息
    // bulk_data[0:channel_len-1] 为频道名
    // bulk_data[channel_len:channel_len+message_len-1] 为消息
    unsigned char bulk_data[8]; /* defined as 8 just for alignment concerns. */
} clusterMsgDataPublish;
 
typedef struct {
    // 节点的配置纪元
    uint64_t configEpoch; /* Config epoch of the specified instance. */
    // 节点的名字
    char nodename[REDIS_CLUSTER_NAMELEN]; /* Name of the slots owner. */
    // 节点的槽布局
    unsigned char slots[REDIS_CLUSTER_SLOTS/8]; /* Slots bitmap. */
} clusterMsgDataUpdate;
 
union clusterMsgData {
    /* PING, MEET and PONG */
    struct {
        /* Array of N clusterMsgDataGossip structures */
        clusterMsgDataGossip gossip[1];
    } ping;
    /* FAIL */
    struct {
        clusterMsgDataFail about;
    } fail;
    /* PUBLISH */
    struct {
        clusterMsgDataPublish msg;
    } publish;
    /* UPDATE */
    struct {
        clusterMsgDataUpdate nodecfg;
    } update;
};
```

##### ping/pong

Redis节点会记录其向每一个节点上一次发出ping和收到pong的时间，心跳发送时机与这两个值有关。

通过下面的方式既能保证及时更新集群状态，又不至于使心跳数过多：

- 集群内每个节点维护定时任务默认间隔1秒，每秒执行10次，定时任务里每秒随机选取5个节点，找出最久没有通信的节点发送ping消息，用于保证 Gossip信息交换的随机性
- 同时每100毫秒都会扫描本地节点列表，如果发现节点最近一次接受pong消息的时间大于cluster_node_timeout/2，则立刻发送ping消息，防止该节点信息太长时间未更新。

- 收到ping或meet，立即回复pong

相关的参数`cluster_node_timeout`

`cluster_node_timeout`参数对消息发送的节点数量影响非常大。当带宽资源紧张时，可以适当调大这个参数，如从默认15秒改为30秒来降低带宽占用率。过度调大`cluster_node_timeout `会影响消息交换的频率从而影响故障转移、槽信息更新、新节点发现的速度。因此需要根据业务容忍度和资源消耗进行平衡。

##### meet

新节点加入时，处理过程如下：

- 发送meet包加入集群
- 从pong包中的gossip得到未知的其他节点
- 循环上述过程，直到最终加入集群

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/redis-cluster-1.png)

消息体在Redis内部采用`clusterMsg Data` 结构声明，定义发送消息的数据。其中ping、meet、pong都采用clusterMsgDataGossip数组作为消息体数据，实际消息类型使用消息头的type属性区分。每个消息体包含该节点的多个clusterMsgDataGossip结构数据，用于信息交换。

当接收到ping、meet消息时,接收节点会解析消息内容并根据自身的识别情况做出相应处理。

#### 故障转移（高可用）

##### 故障发现



##### 故障恢复





#### 数据访问

Redis cluster采用去中心化的架构，集群的主节点各自负责一部分槽，客户端如何确定key到底会映射到哪个节点上呢？

在cluster模式下，**节点对请求的处理过程**如下：

- 检查当前key是否存在当前NODE？ 
  - 通过crc16（key）/16384计算出slot
  - 查询负责该slot负责的节点，得到节点指针
  - 该指针与自身节点比较
- 若slot不是由自身负责，则返回MOVED重定向
- 若slot由自身负责，且key在slot中，则返回该key对应结果
- 若key不存在此slot中，检查该slot是否正在迁出（MIGRATING）？
- 若key正在迁出，返回ASK错误重定向客户端到迁移的目的服务器上
- 若Slot未迁出，检查Slot是否导入中？
- 若Slot导入中且有ASKING标记，则直接操作
- 否则返回MOVED重定向

##### MOVED重定向

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/redis-cluster-3.png)

- 槽命中：直接返回结果
- 槽不命中：即当前键命令所请求的键不在当前请求的节点中，则当前节点会向客户端发送一个Moved 重定向，客户端根据Moved 重定向所包含的内容找到目标节点，再一次发送命令。

##### ACK重定向

Ask重定向发生于集群伸缩时，集群伸缩会导致槽迁移，当我们去源节点访问时，此时数据已经可能已经迁移到了目标节点，使用Ask重定向来解决此种情况。

![img](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/redis-cluster-5.png)



### 总结

#### Redis集群和Redis主从架构的区别？

- Redis主从指的是一主多从，Redis集群指的是多主多从。
- 

#### Redis集群功能限制

Redis集群相对单机在功能上存在一些限制，需要开发人员提前了解，在使用时做好规避。限制如下:

1、 key批量操作支持有限。如mset、mget，目前只支持具有相同slot值的key执行批量操作。对于映射为不同slot值的key由于执行mget、mget等操作可能存在于多个节点上因此不被支持。

2、key事务操作支持有限。同理只支持多key在同一节点上的事务操作，当多个key分布在不同的节点上时无法使用事务功能。

3、key作为数据分区的最小粒度，因此不能将一个大的键值对象如hash、list等映射到不同的节点。

4、不支持多数据库空间。单机下的Redis可以支持16个数据库，集群模式下只能使用一个数据库空间,即 db 0。

5、复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构。

## 内存淘汰机制

Redis共支持八种淘汰策略，分别是noeviction、volatile-random、volatile-ttl、volatile-lru、volatile-lfu、allkeys-lru、allkeys-random 和 allkeys-lfu 策略，可以分成三类看。

![image-20230812190309584](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230812190309584.png)

### 不淘汰策略 

#### noeviction

Redis4.0后新增的淘汰策略，该策略是Redis的默认策略。

在这种策略下，一旦缓存被写满了，再有写请求来时，Redis 不再提供服务，而是直接返回错误。这种策略不会淘汰数据，所以无法解决缓存污染问题。一般生产环境不建议使用。

### 对设置了过期时间的数据进行淘汰 

#### volatile-random

在设置了过期时间的键值对中，进行随机删除。因为是随机删除，无法把不再访问的数据筛选出来，所以可能依然会存在缓存污染现象，无法解决缓存污染问题。

#### Volatile-ttl

Redis在筛选需删除的数据时，越早过期的数据越优先被选择。

#### volatile-lru

##### LRU算法

LRU 算法的全称是 Least Recently Used，按照最近使用的原则来筛选数据，最近使用的数据会留在缓存中，是一种按照访问时间进行淘汰的算法。

**LRU 策略的核心思想：如果一个数据刚刚被访问，那么这个数据肯定是热数据，还会被再次访问**。

LRU 会把所有的数据组织成一个链表，链表的头和尾分别表示 MRU 端和 LRU 端，分别代表最近最常使用的数据和最近最不常用的数据。

![image-20230812185829269](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230812185829269.png)



##### Redis中LRU实现

LRU 算法在实际实现时，需要用链表管理所有的缓存数据，这会带来额外的空间开销。而且，当有数据被访问时，需要在链表上把该数据移动到 MRU 端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。所以，在 Redis 中，LRU 算法被做了简化，以减轻数据淘汰对缓存性能的影响。

Redis 在实现 LRU 策略时使用了两个近似方法：

1、RedisObject 结构中设置了一个 lru 字段，用来记录数据的访问时间戳；

2、Redis 并没有为所有的数据维护一个全局的链表，而是通过随机采样方式，选取一定数量的数据放入候选集合，后续在候选集合中根据 lru 字段值的大小进行筛选。

具体淘汰过程为：

1. 第一次会随机选出 N 个数据，把它们作为一个候选集合。接下来，Redis 会比较这 N 个数据的 lru 字段，把 lru 字段值最小的数据从缓存中淘汰出去。
2. 当需要再次淘汰数据时，Redis 需要挑选数据进入第一次淘汰时创建的候选集合。这儿的挑选标准是：能进入候选集合的数据的 lru 字段值必须小于候选集合中最小的 lru 值。当有新数据进入候选数据集后，如果候选数据集中的数据个数达到了 maxmemory-samples，Redis 就把候选数据集中 lru 字段值最小的数据淘汰出去。这样一来，Redis 缓存不用为所有的数据维护一个大链表，也不用在每次数据访问时都移动链表项，提升了缓存的性能。

![image-20230812185405802](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230812185405802.png)

Redis 提供了一个配置参数 maxmemory-samples，这个参数就是 Redis 选出的数据个数 N。例如，执行如下命令，可以让 Redis 选出 100 个数据作为候选数据集：

> CONFIG SET maxmemory-samples 100

##### LRU的优劣

优势：

在数据被频繁访问的业务场景中，LRU 策略的确能有效留存访问时间最近的数据。而且，因为留存的这些数据还会被再次访问，所以又可以提升业务应用的访问速度。

劣势：

1. **只看数据的访问时间**，使用 LRU 策略在处理扫描式单次查询操作时，无法解决缓存污染。
2. 临时数据可能会取代真正经常使用的数据。比如，短时间内，大量临时数据涌入 redis，而触发发生内存淘汰，可能会将那些真正经常使用的数据驱逐。

> 扫描式单次查询操作，就是指应用对大量的数据进行一次全体读取，每个数据都会被读取，而且只会被读取一次。此时，因为这些被查询的数据刚刚被访问过，所以 lru 字段值都很大。

#### volatile-lfu

LFU 算法的全称是 Least Frequently Used，也就是每次淘汰那些使用次数最少的数据，是一种按照访问频次进行淘汰的算法。

##### LFU算法

LFU将数据和数据的访问频次保存在一个容量有限的容器中，当访问一个数据时：

1. 该数据在容器中，则将该数据的访问频次加1。
2. 该数据不在容器中，则将该数据加入到容器中，且访问频次为1。

当数据量达到容器的限制后，会剔除掉访问频次最低的数据。

![image-20230812193611958](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230812193611958.png)

**常规LFU算法面临的问题**

在数据请求模式比较稳定（没有对于某个数据突发的高频访问这样的不稳定模式）的情况下，LFU的表现还是很不错的。

但在数据的请求模式大多不稳定的情况下，LFU一般会有这样一些问题：

1. **热点数据问题**：热点数据一般只是几天内有较高的访问频次，过了这段时间就没那么大意义去缓存了。但是因为在热点期间他的频次被刷上去了，导致之后很长一段时间内很难被淘汰；
2. **新增数据问题**：如果采用只记录缓存中的数据的访问信息，新加入的高频访问数据在刚加入的时候由于没有累积优势，很容易被淘汰掉；
3. **空间问题**：如果记录全部出现过的数据的访问信息，会占用更多的内存空间。

##### Redis中LFU实现

Redis 在实现 LFU 策略的时候，同样采用了近似的实现方法，引入了三个策略：

1. 概率量级计数：非线性递增的技术方法节省计数器字段空间，可配参数server.lfu_log_factor，影响计数的量级范围，整计数器counter的增长速度，lfu-log-factor越大，counter增长的越慢。
2. 计数衰减：可以解决热点数据问题，配置参数server.lfu-decay-time，控制LFU计数衰减，是一个以分钟为单位的数值，可以调整counter的减少速度。
3. 复用LRU字段，把原来 24bit 大小的 lru 字段，又进一步拆分成了两部分:
   - ldt 值：lru 字段的前 16bit，表示数据的访问时间戳；
   - counter 值：lru 字段的后 8bit，表示数据的访问次数。

当 LFU 策略筛选数据时，Redis 会在候选集合中，根据数据 lru 字段的后 8bit选择访问次数最少的数据进行淘汰。当访问次数相同时，再根据 lru 字段的前 16bit 值大小，选择访问时间最久远的数据进行淘汰。

**（一）计数衰减**

某个key的counter被衰减的时机是在它被访问的时候。计数衰减的触发也是被动的，而非Redis主动或者定时触发的。在缓存被访问时，会更新数据的访问计数，更新的步骤是：

1. 先在现有数据的计数上进行计数衰减。
2. 再对完成衰减后的计数进行概率增加。

计数衰减的思想为：可配参数server.lfu-decay-time所代表的含义是计数衰减的周期长度，单位是分钟。**当时间过去一个周期（也就是lfu-decay-time分钟），计数值就会减1**。基于这样的思想，其具体过程为：

1. redisObject结构中的lru字段的高16bit，记录的是该key上次进行衰减的时间。通过**(当前时间-上次衰减的时间）/ （周期单位）**即为key需要将counter衰减的数量n。
2. 通过`LFUDecrAndReturn`方法得到该key的counter，将counter=counter-n。

**（二）概率量级计数**

Redis中给counter配置了8bit的存储空间，也就是counter最大值为255：

1. 当counter等于最大值 255 时，不再增加counter。
2. 当counter小于 255 时，Redis会计算一个阈值 p，以及一个取值为 0 到 1 之间的随机概率值 r。如果概率 r 小于阈值 p，counter加 1；否则counter不变。

其中，`p=1/(counter*factor+1)`，可配参数server.lfu_log_factor越大时，概率p在同等情况下则会越低，counter字段8 bit一共255的上限也就越不容易被触达，换句话说，factor越大，Redis的counter字段能够记录的访问频次量级也就越高。

**（三）key的初始化及更新**

1、key被创建时，对 redisObject 结构体中的 lru 变量初始化值，会由两部分组成：

- ldt值设置为当前时间。
- counter值，被设置为宏定义 LFU_INIT_VAL，默认值为 5。

2、key被访问时更新变量值：

- 根据距离上次访问的时长，衰减访问计数
- 根据当前访问次数更新访问counter值
- 调用 LFUGetTimeInMinutes 函数，来获取当前的时间戳，并和更新后的访问次数组合，形成最新的访问频率信息，赋值给键值对的 lru 变量。

##### LFU的优劣

优点：通过key的访问频率和访问时间比较来淘汰key，重点突出的是Frequently Used，用于在缓存容量有限时决定哪些缓存块应该被清除，避免了LRU算法的明显缺陷。

缺点：最近加入的数据总是易于被剔除（缓存末端抖动），因为他起始的频率很低。

### 对全部数据进行淘汰 

#### allkeys-random

从所有键值对中随机选择并删除数据。

volatile-random 跟 allkeys-random算法一样。

#### allkeys-lru

使用 LRU 算法在所有数据中进行筛选。

具体LFU算法跟上述 volatile-lru 中介绍的一致，只是筛选的数据范围是全部缓存。

#### allkeys-lfu

使用 LFU算法在所有数据中进行筛选。

具体LFU算法跟上述 volatile-lfu 中介绍的一致，只是筛选的数据范围是全部缓存。

### 淘汰过程

![image-20230812221333276](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230812221333276.png)

真正的淘汰包括下面的内容：

- 何时清理
- 如何清理，清理哪些 --- 依据淘汰策略指定
- 清理多少

#### 清理时机

每一条客户端的请求处理之后，看是否有必要进行内存淘汰。如果需要，走淘汰逻辑，此时分两种情况：

1. 淘汰数据少：这种很理想，一次性可以搞定。
2. 淘汰数据多：如果数据过多，为避免长时间阻塞，提供了一些可配置的限制，如果达到限制条件还没有清理完成，暂时放入到时间事件中，等待下一轮清理。

#### 清理多少

待清理的大小 = used - maxmemory，即当前使用内存大小 - 设定的阈值。

一次性没处理完（可以设置一定时长限制），可以扔到时间事件中，周期性的处理，直到达到目标 ...

清理的目标是使得当前内存小于maxmemory

## 过期删除机制

Redis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。

### 设置过期时间

设置 key 过期时间的命令一共有 4 个：

- `expire key n  `：设置 key 在 n 秒后过期；
- `pexpire key n`：设置 key 在 n 毫秒后过期;
- `expireat key n  `：设置 key 在某个时间戳（精确到秒）之后过期；
- `pexpireat key n`：设置 key 在某个时间戳（精确到毫秒）之后过期；

当然，在设置字符串时，也可以同时对 key 设置过期时间，共有 3 种命令：

- `set key value ex n ` ：设置键值对的时候，同时指定过期时间（精确到秒）；
- `set key value px n ` ：设置键值对的时候，同时指定过期时间（精确到毫秒）；
- `setex key n value   ` ：设置键值对的时候，同时指定过期时间（精确到秒）。

### 判断key过期

当对一个 key 设置了过期时间时，Redis 会把该 key 带上过期时间存储到一个**过期字典**（expires dict）中，其数据结构如下。

![image-20230812224425577](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230812224425577.png)

其中，过期字典的 key 是一个指针，指向某个键对象；过期字典的 value 是一个 long long 类型的整数，这个整数保存了 key 的过期时间；

当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中：

- 如果不在，则正常读取键值；
- 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。

### 过期删除策略

#### 定时删除

定时删除策略的做法是，**在设置 key 的过期时间时，同时创建一个定时事件，当时间到达时，由事件处理器自动执行 key 的删除操作。**

定时删除策略的**优点**：

- 可以保证过期 key 会被尽快删除，也就是内存可以被尽快地释放。因此，定时删除对内存是最友好的。

定时删除策略的**缺点**：

- 在过期 key 比较多的情况下，删除过期 key 可能会占用相当一部分 CPU 时间，在内存不紧张但 CPU 时间紧张的情况下，将 CPU 时间用于删除和当前任务无关的过期键上，无疑会对服务器的响应时间和吞吐量造成影响。所以，定时删除策略对 CPU 不友好。

#### 惰性删除

惰性删除策略的做法是，**不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。**

惰性删除策略的**优点**：

- 因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。

惰性删除策略的**缺点**：

- 如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好。

#### 定期删除

定期删除策略的做法是，**每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。**

定期删除策略的**优点**：

- 通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。

定期删除策略的**缺点**：

- 内存清理方面没有定时删除效果好，同时没有惰性删除使用的系统资源少。
- 难以确定删除操作执行的时长和频率。如果执行的太频繁，定期删除策略变得和定时删除策略一样，对CPU不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。

#### Redis中使用的删除策略

前面介绍了三种过期删除策略，每一种都有优缺点，仅使用某一个策略都不能满足实际需求。

**Redis 选择了「惰性删除+定期删除」这两种策略配合使用**。



**惰性删除**

Redis 在访问或者修改 key 之前，都会调用 expireIfNeeded 函数对其进行检查，检查 key 是否过期：

1. 如果过期，则删除该 key，至于选择异步删除，还是选择同步删除，根据 `lazyfree_lazy_expire` 参数配置决定（Redis 4.0版本开始提供参数），然后返回 null 客户端；

   > server.lazyfree_lazy_expire 为 1 表示异步删除，不为1时表示同步删除

2. 如果没有过期，不做任何处理，然后返回正常的键值对给客户端；



**定期删除**

每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。

1、这个间隔检查的时间是多长呢？

在 Redis 中，默认每秒进行 10 次过期检查一次数据库，此配置可通过 Redis 的配置文件 redis.conf 进行配置，配置键为 hz ，默认值是 10。

特别强调下，每次检查数据库并不是遍历过期字典中的所有 key，而是从数据库中随机抽取一定数量的 key 进行过期检查。

2、随机抽查的数量是多少呢？

定期删除的实现在 expire.c 文件下的 `activeExpireCycle` 函数中，其中随机抽查的数量由 `ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP` 定义的，它是写死在代码中的，数值是 20。也就是说，数据库每轮抽查时，会随机选择 20 个 key 判断是否过期。

3、删除流程是什么样的？

	1. 从过期字典中随机抽取 20 个 key；
 	2. 检查这 20 个 key 是否过期，并删除已过期的 key；
 	3. 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。可以看到，定期删除是一个循环的流程。Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。

![image-20230813160316780](Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6.assets/image-20230813160316780.png)

### 持久化处理过期

#### RDB

**1 从内存数据库持久化数据到RDB文件**

持久化key之前，会检查是否过期，过期的key不进入RDB文件 

**2 从RDB文件恢复数据到内存数据库**

数据载入数据库之前，会对key先进行过期检查，如果过期，不导入数据库

#### AOF

**1 从内存数据库持久化数据到AOF文件**

当key过期后，还没有被删除，此时进行执行持久化操作（该key是不会进入aof文件的，因为没有发生修改命令） 当key过期后，在发生删除操作时，程序会向aof文件追加一条del命令（在将来的以aof文件恢复数据的时候该过期的键就会被删掉） AOF重写：重写时，会先判断key是否过期，已过期的key不会重写到aof文件

### 过期删除机制和内存淘汰机制

如果缓存中的数据永久存在，那占用的内存就会变得越来越大，而内存是有限的，所以缓存系统需要在需要的时候删除一些不必要的缓存数据以节约内存空间。

Redis提供了两种机制配合来达到上述目的：**过期删除机制**和**内存淘汰机制**。

因此我们可以得出两种的机制的关系：

1. 相似性：都是为了清理Redis中的缓存，释放内存的机制。
2. 不同性：过期删除机制的目的是删除掉缓存中过期的key，清理的数据是已经过期的数据，是无用的。内存淘汰机制是在当前内存超出阈值的时候，通过算法选择出要淘汰的key，此时的数据是还未过期的，选择出相对价值小的数据淘汰掉。

